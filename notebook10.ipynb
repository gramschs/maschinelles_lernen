{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66eba366",
   "metadata": {},
   "source": [
    "# 10. Support Vector Machines\n",
    "\n",
    "Das maschinelle Lernverfahren Support Vector Machines gehört zu den überwachten\n",
    "Lernverfahren. Sie können sowohl für Klassifikations- also auch\n",
    "Regressionsprobleme eingesetzt werden. Prinzipiell könnten wir den Namen Support\n",
    "Vector Machines ins Deutsche übersetzen, also das Verfahren als\n",
    "Stützvektor-Maschine bezeichnen. Jedoch ist der deutsche Begriff so unüblich,\n",
    "dass wir beim englischsprachigen Begriff bleiben oder einfach die Abkürzung SVM\n",
    "verwenden. Zur Einführung der SVMs betrachten wir ein binäres\n",
    "Klassifikationsproblem.\n",
    "\n",
    "## 10.1 Maximiere den Rand, aber soft\n",
    "\n",
    "### Lernziele 10.1\n",
    "\n",
    "* Sie kennen die Abkürzung **SVM** für **Support Vector Machines**.\n",
    "* Sie kennen die Idee, bei Support Vector Machines den **Margin** (=\n",
    "  Randabstand) zu maximieren.\n",
    "* Sie wissen, was Stützvektoren bzw. **Support Vectors** sind.\n",
    "* Sie wissen, dass ein harter Randabstand nur bei linear trennbaren Datensätzen\n",
    "  möglich ist.\n",
    "* Sie wissen, dass eigentlich nicht trennbare Datensätzen mit der Technik **Soft\n",
    "  Margin** (= weicher Randabstand) dennoch klassifiziert werden können.\n",
    "\n",
    "### Welche Trenn-Gerade soll es sein?\n",
    "\n",
    "Support Vector Machines (SVM) können sowohl für Klassifikations- als auch\n",
    "Regressionsprobleme genutzt werden. Insbesondere wenn viele Merkmale (Features)\n",
    "vorliegen, sind SVMs gut geeignet. Auch neigen SVMs weniger zu Overfitting.\n",
    "Daher lohnt es sich, Support Vector Machines anzusehen.\n",
    "\n",
    "Warum es weniger zu Overfitting neigt und mit Ausreißern besser umgehen kann,\n",
    "sehen wir bereits an der zugrundeliegenden Idee, die hinter dem Verfahren\n",
    "steckt. Um das Basis-Konzept der SVMs zu erläutern, besorgen wir uns zunächst\n",
    "künstliche Messdaten. Dazu verwenden wir die Funktion `make_blobs` aus dem\n",
    "Scikit-Learn-Modul. Mehr Details zum Aufruf der Funktion finden Sie in der\n",
    "[Scikit-Learn-Dokumentation/make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html?highlight=make+blobs#sklearn.datasets.make_blobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate artificial data\n",
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.50)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3644ee9",
   "metadata": {},
   "source": [
    "Die Funktion `make_blobs` erzeugt standardmäßig zwei Input-Features, da die\n",
    "Option `n_features` auf den Wert 2 voreingestellt ist, und einen Output, bei dem\n",
    "die Labels entweder durch 0 oder 1 gekennzeichnet sind. Durch die Option\n",
    "`random_state=0` wird der Zufall ausgeschaltet.\n",
    "\n",
    "Wenn wir die Daten in einen Pandas-DataFrame packen und anschließend\n",
    "visualisieren, erhalten wir folgenden Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import plotly.express as px\n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Feature 1': X[:,0],\n",
    "    'Feature 2': X[:,1],\n",
    "    'Status': y.astype(bool),\n",
    "    })\n",
    "\n",
    "fig = px.scatter(daten, x = 'Feature 1', y = 'Feature 2',  color='Status',\n",
    "                 title='Künstliche Daten', color_discrete_sequence=['#b40426','#3b4cc0'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1c342",
   "metadata": {},
   "source": [
    "Wir können uns jetzt verschiedene Geraden vorstellen, die die blauen Punkte von\n",
    "den roten Punkten trennen. In der folgenden Grafik sind drei eingezeichnet.\n",
    "Welche würden Sie nehmen und warum?\n",
    "\n",
    "![verschiedene Trenngeraden in einem Scatterplot](https://gramschs.github.io/book_ml4ing/_images/fig10_01_annotated.pdf)\n",
    "\n",
    "Alle drei Geraden trennen die blauen von den roten Punkten. Jedoch könnte Gerade\n",
    "3 problematisch werden, wenn beispielsweise ein neuer blauer Datenpunkt an der\n",
    "Position (2.3, 3.3) dazukäme. Dann würde Gerade 3 diesen Punkt als rot\n",
    "klassifizieren. Ähnlich verhält es sich mit Gerade 1. Ein neuer blauer\n",
    "Datenpunkt an der Position (0.5, 3) würde fälschlicherweise als rot\n",
    "klassifiziert werden. Gerade 2 bietet den sichersten Abstand zu den bereits\n",
    "vorhandenen Datenpunkten. Wir können diesen \"Sicherheitsstreifen\" folgendermaßen\n",
    "visualisieren.\n",
    "\n",
    "![Sicherheitsstreifen](https://gramschs.github.io/book_ml4ing/_images/fig10_02_annotated.pdf)\n",
    "\n",
    "Der Support-Vector-Algorithmus sucht nun die Gerade, die die Datenpunkte mit dem\n",
    "größten Randabstand (= Margin) voneinander trennt. Im Englischen sprechen wir\n",
    "daher auch von **Large Margin Classification**. Die Suche nach dieser Geraden\n",
    "ist dabei etwas zeitaufwändiger als die Berechnung der Gewichte bei der\n",
    "logistischen Regression. Wenn aber einmal das Modell trainiert ist, ist die\n",
    "Prognose effizienter, da nur die sogenannten **Stützvektoren**, auf englisch\n",
    "**Support Vectors** gespeichert und ausgewertet werden. Die Stützvektoren sind\n",
    "die Vektoren, die vom Ursprung des Koordinatensystems zu den Punkten zeigen, die\n",
    "auf der Grenze des Sicherheitsbereichs liegen.  \n",
    "\n",
    "![Stützvektoren eingezeichnet](https://gramschs.github.io/book_ml4ing/_images/fig10_03.pdf)\n",
    "\n",
    "### Großer, aber weicher Randabstand\n",
    "\n",
    "Bei dem oben betrachteten Beispiel lassen sich blaue und rote Datenpunkte\n",
    "komplett voneinander trennen. Für den Fall, dass einige wenige Datenpunkte\n",
    "\"falsch\" liegen, erlauben wir Ausnahmen. Wie viele Ausnahmen wir erlauben\n",
    "wollen, die im Sicherheitsstreifen liegen, steuern wir mit dem Parameter `C`.\n",
    "Ein großes `C` bedeutet, dass wir eine große Mauer an den Grenzen des\n",
    "Sicherheitsabstandes errichten. Es kommt kaum vor, dass Datenpunkte innerhalb\n",
    "des Margins liegen. Je kleiner `C` wird, desto mehr Datenpunkte sind innerhalb\n",
    "des Sicherheitsbereichs erlaubt.\n",
    "\n",
    "Im Folgenden betrachten wir einen neuen künstlichen Datensatz, bei dem die\n",
    "blauen von den roten Punkte nicht mehr ganz so stark getrennt sind. Schauen Sie\n",
    "sich die fünf verschiedenen Margins an, die entstehen, wenn der Parameter `C`\n",
    "variiert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e54098",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('../assets/chapter10/fig04.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e02d5",
   "metadata": {},
   "source": [
    "### Zusammenfassung Kapitel 10.1\n",
    "\n",
    "In diesem Abschnitt haben wir die Ideen kennengelernt, die den Support Vector\n",
    "Machines zugrunde liegen. Im nächsten Abschnitt schauen wir uns an, wie ein\n",
    "SVM-Modell mit Scikit-Learn trainiert wird.\n",
    "\n",
    "## 10.2 Training SVM mit Scikit-Learn\n",
    "\n",
    "### Lernziele Kapitel 10.2\n",
    "\n",
    "* Sie können ein SVM-Modell mit Scikit-Learn trainieren.\n",
    "\n",
    "### Scikit-Learn bietet mehrere Implementierungen\n",
    "\n",
    "Wenn wir in der Dokumentation von Scikit-Learn\n",
    "[Scikit-Learn/SVM](https://scikit-learn.org/stable/modules/svm.html) die Support\n",
    "Vector Machines nachschlagen, so finden wir drei Einträge\n",
    "\n",
    "* SVC,\n",
    "* NuSVC und\n",
    "* LinearSVC.\n",
    "\n",
    "Die Beispiele des letzten Abschnittes sind linearer Natur, so dass sich\n",
    "eigentlich die Klasse \"LinearSVC\" aus Effiziengründen anbieten würde. Da wir\n",
    "aber im nächsten Abschnitt uns auch mit nichtlinearen Problemen beschäftigen\n",
    "werden, fokussieren wir uns gleich auf den SVC-Algorithmus mit seinen Optionen.\n",
    "NuSVC ist ähnlich zu SVC, bietet aber die zusätzliche Möglichkeit, die Anzahl\n",
    "der Stützvektoren einzuschränken.\n",
    "\n",
    "Vielleicht wundern Sie sich, dass die Klasse SVC und nicht SVM heißt. Das C in\n",
    "SVC soll deutlich machen, dass wir die Support Vector Machines nutzen wollen, um\n",
    "ein Klassifikationsproblem (= Classification Problem) zu lösen.\n",
    "\n",
    "### Training mit fit und score\n",
    "\n",
    "Zuerst importieren wir aus Scikit-Learn das entsprechende Modul 'SVM' und\n",
    "instantiieren ein Modell. Da wir die etwas allgemeinere Klasse SVC anstatt\n",
    "LinearSVC verwenden, müssen wir bereits bei der Erzeugung die Option `kernel=`\n",
    "auf linear setzen, also `kernel='linear'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_modell = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9ba25",
   "metadata": {},
   "source": [
    "Wir erzeugen uns erneut künstliche Messdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pylab as plt; plt.style.use('bmh')\n",
    "\n",
    "# generate artificial data\n",
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.50)\n",
    "\n",
    "# plot artificial data\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n",
    "                 title='Künstliche Daten',\n",
    "                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777b0fa",
   "metadata": {},
   "source": [
    "Als nächstes teilen wir die Messdaten in Trainings- und Testdaten auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1729b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f293114",
   "metadata": {},
   "source": [
    "Nun können wir unser SVM-Modell trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8112bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_modell.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886554cb",
   "metadata": {},
   "source": [
    "Und als nächstes analysieren, wie viele der Testdaten mit dem trainierten Modell\n",
    "korrekt klassifiziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_modell.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa31920",
   "metadata": {},
   "source": [
    "Ein super Ergebnis! Schön wäre es jetzt noch, die gefundene Trenngerade zu\n",
    "visualisieren. Dazu modifizieren wir einen Code-Schnippsel aus dem Buch: »Data\n",
    "Science mit Python« von Jake VanderPlas (mitp Verlag 2017), ISBN 978-3-95845-\n",
    "695-2, siehe\n",
    "[https://github.com/jakevdp/PythonDataScienceHandbook](https://github.com/jakevdp/PythonDataScienceHandbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec62c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: VanderPlas \"Data Science mit Python\", S. 482\n",
    "# modified by Simone Gramsch\n",
    "import numpy as np\n",
    "\n",
    "def plot_svc_grenze(model):\n",
    "    # aktuelles Grafik-Fenster auswerten\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    # Raster für die Auswertung erstellen\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    # Entscheidungsgrenzen und Margins darstellen\n",
    "    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    # Stützvektoren darstellen\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none', edgecolors='orange');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13af012",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Soft Margin');\n",
    "\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3885871",
   "metadata": {},
   "source": [
    "### Der Parameter C\n",
    "\n",
    "Im letzten Abschnitt haben wir uns mit dem Parameter `C` beschäftigt, der\n",
    "Ausnahmen innerhalb des Sicherheitsstreifens erlaubt. Ein großes `C` bedeutet\n",
    "ja, dass die Wand des Margins hoch ist und kaum (oder gar keine) Punkte\n",
    "innerhalb des Sicherheitsstreifens liegen dürfen. Als nächstes schauen wir uns\n",
    "an, wie der Parameter `C` gesetzt wird.  \n",
    "\n",
    "Die Option zum Setzen des Parameters C lautet schlicht und einfach `C=`. Dabei\n",
    "muss C immer positiv sein.\n",
    "\n",
    "Damit aber besser sichtbar wird, wie sich C auswirkt, vermischen wir die\n",
    "künstlichen Daten stärker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate artificial data\n",
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.80)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# plot artificial data\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n",
    "                 title='Künstliche Daten',\n",
    "                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahl des Modells mit linearem Kern und großem C\n",
    "svm_modell = svm.SVC(kernel='linear', C=1000000)\n",
    "\n",
    "# Training und Bewertung\n",
    "svm_modell.fit(X_train, y_train);\n",
    "svm_modell.score(X_test, y_test)\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Soft Margin');\n",
    "plot_svc_grenze(svm_modell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01844dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahl des Modells mit linearem Kern und kleinem C\n",
    "svm_modell = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "# Training und Bewertung\n",
    "svm_modell.fit(X_train, y_train);\n",
    "svm_modell.score(X_test, y_test)\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Soft Margin');\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac5a7f",
   "metadata": {},
   "source": [
    "### Zusammenfassung Kapitel 10.2\n",
    "\n",
    "Verwenden wir den SVC-Klassifikator aus dem Modul SVM von Scikit-Learn, können\n",
    "wir mittels der Option `kernel='linear'` eine binäre Klassifikation durchführen,\n",
    "bei der die Trennungsgerade den größtmöglichen Abstand zwischen den\n",
    "Punkteclustern erzeugt, also einen möglichst großen Margin. Sind die Daten nicht\n",
    "linear trennbar, so können wir mit der Option `C=` steuern, wie viele Ausnahmen\n",
    "erlaubt werden sollen. Mit Ausnahmen sind Punkte innerhalb des Margins gemeint.\n",
    "Im nächsten Abschnitt betrachten wir nichtlineare Trennungsgrenzen.\n",
    "\n",
    "## 10.3 Nichtlineare SVM\n",
    "\n",
    "### Lernziele Kapitel 10.3\n",
    "\n",
    "* Sie kennen den **Kernel-Trick**.\n",
    "* Sie können mit den **radialen Basisfunktionen** als neue Option für\n",
    "  SVM-Verfahren nichtlinear trennbare Daten klassifizieren.\n",
    "\n",
    "### Nichtlineare trennbare Daten\n",
    "\n",
    "Für die Support Vector Machines sind wir bisher davon ausgegangen, dass die\n",
    "Daten -- ggf. bis auf wenige Ausnahmen -- linear getrennt werden können. Im\n",
    "Folgenden betrachten wir nun einen künstlichen Messdatensatz, bei dem das\n",
    "offensichtlich nicht geht. Dazu nutzen wir die in Scikit-Learn integrierte\n",
    "Funktion `make_circles()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# künstliche Messdaten generieren\n",
    "X,y = make_circles(100, random_state=0, factor=0.3, noise=0.1)\n",
    "\n",
    "# künstliche Messdaten visualisieren\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n",
    "                 title='Künstliche Daten',\n",
    "                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a36e4",
   "metadata": {},
   "source": [
    "Das menschliche Auge erkennt sofort das Muster in den Daten. Ganz offensichtlich\n",
    "sind die roten und blauen Punkte kreisförmig angeordnet und können\n",
    "dementsprechend auch durch einen Kreis getrennt werden. Allerdings wird ein\n",
    "SVM-Klassifikator, so wie wir das SVM-Verfahren bisher kennengelernt haben,\n",
    "versagen. Eine Gerade zur Klassifikation der roten und blauen Punkte passt\n",
    "einfach nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29c919",
   "metadata": {},
   "source": [
    "### Aus 2 mach 3\n",
    "\n",
    "Die Idee zur Überwindung dieses Problems klingt zunächst einmal absurd. Wir\n",
    "machen aus zwei Features drei Features. Als drittes Feature wählen wir das\n",
    "Quadrat des Abstandes eines Punktes zum Ursprung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Extraktion der Daten, damit leichter darauf zugegriffen werden kann\n",
    "X1 = X[:,0]\n",
    "X2 = X[:,1]\n",
    "\n",
    "# neues Feature als Quadrat des Abstandes zum Ursprung\n",
    "X3 = np.sqrt( X1**2 + X2**2 )\n",
    "\n",
    "fig = px.scatter_3d(x=X1, y=X2, z=X3, color=y, color_continuous_scale=['#3b4cc0', '#b40426'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd9006",
   "metadata": {},
   "source": [
    "Bitte drehen Sie die Ansicht solange, bis die z-Achse nach oben zeigt. Die\n",
    "Punkte bilden eine Art Paraboloiden. In dieser neuen Ansicht können wir eine\n",
    "Ebene finden, die die roten von den blauen Punkten trennt.\n",
    "\n",
    "In der folgenden Grafik ist eine Trennebene eingezeichnet. Wenn wir nun den\n",
    "Schnitt der Trennebene mit dem Paraboloiden bilden, entsteht eine Kreislinie.\n",
    "Drehen wir wieder unsere Ansicht zurück, so dass wir von oben auf die\n",
    "X1-X2-Feature-Ebene blicken, so ist dieser Kreis genau das, was wir auch als\n",
    "Menschen genommen hätten, um die roten von den blauen Punkten zu trennen.\n",
    "\n",
    "![3D-Scatterplot mit Trenngerade](https://gramschs.github.io/book_ml4ing/_images/fig10_06_with_plane.png)\n",
    "\n",
    "![Trennebene](https://gramschs.github.io/book_ml4ing/_images/fig10_07_with_circle.png)\n",
    "\n",
    "### Kernel-Trick\n",
    "\n",
    "Bei diesem künstlichen Datensatz hat das Quadrat der Abstände zum Ursprung als\n",
    "neues Feature sehr gut funktioniert. Das lag aber unter anderem daran, dass die\n",
    "Punkte tatsächlich in Kreisen um den Ursprung verteilt waren. Was ist, wenn das\n",
    "nicht der Fall ist? Wenn der Schwerpunkt der Kreise verschoben wäre, müssten wir\n",
    "auch die Transformationsfunktion zum Erzeugen des dritten Features in diesen\n",
    "Schwerpunkt verschieben.\n",
    "\n",
    "Glücklicherweise übernimmt Scikit-Learn für uns die Suche nach einer passenden\n",
    "Transformationsfunktion automatisch. Das Verfahren, das dazu in die\n",
    "SVM-Algorithmen eingebaut ist, wird **Kernel-Trick** genannt. Es beruht darauf,\n",
    "dass manche Funktionen in ein Skalarprodukt umgewandelt werden können. Und dann\n",
    "wird nicht das dritte Feature mit der Transformationsfunktion aus den ersten\n",
    "beiden Features berechnet, was sehr zeitaufwendig werden könnte, sondern die\n",
    "Transformationsfunktion wird direkt in das Lernverfahren eingebaut. Da\n",
    "Funktionen, die dafür geeignet sind, werden als **Kernel-Funktionen**\n",
    "bezeichnet.\n",
    "\n",
    "Am häufigsten zum Einsatz kommt dabei die sogenannte **radiale Basisfunktion**.\n",
    "Die radialen Basisfunktionen werden mit **RBF** abgekürzt. Sie haben die tolle\n",
    "Eigenschaft, dass sie nur vom Abstand eines Punktes zum Ursprung abhängen; so\n",
    "wie unser Beispiel oben.\n",
    "\n",
    "Um nichtlinear trennbare Daten zu klassifizieren, nutzen wir in Scikit-Learn das\n",
    "SVC-Lernverfahren. Doch diesmal wählen wir als Kern nicht die linearen\n",
    "Funktionen, sondern die sogenannten radialen Basisfunktionen RBF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50908b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_modell = svm.SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961487bf",
   "metadata": {},
   "source": [
    "Danach erfolgt das Training wie gewohnt mit der `fit()`-Methode, die Bewertung\n",
    "mit der `score()`-Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_modell.fit(X,y);\n",
    "score = svm_modell.score(X,y)\n",
    "\n",
    "print('Score: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b1988",
   "metadata": {},
   "source": [
    "Wir können erneut die Funktion `plot_svc_grenze()`aus dem vorherigen Abschnitt\n",
    "nutzen, um die Stützvektoren mit einem orangefarbenem Kreis zu markieren und die\n",
    "Entscheidungsgrenze zu visualisieren. Durch die radialen Basisfunktionen\n",
    "erhalten wir keinen Kreis, sondern ein deformiertes Ei. Dafür brauchen wir uns\n",
    "aber keine Gedanken über die Wahl der Funktion zu machen, um das neue Feature\n",
    "aus den bisherigen zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: VanderPlas \"Data Science mit Python\", S. 482\n",
    "# modified by Simone Gramsch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pylab as plt; plt.style.use('bmh')\n",
    "\n",
    "def plot_svc_grenze(model):\n",
    "    # aktuelles Grafik-Fenster auswerten\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    # Raster für die Auswertung erstellen\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    # Entscheidungsgrenzen und Margins darstellen\n",
    "    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    # Stützvektoren darstellen\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none', edgecolors='orange');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b818ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X1, X2, c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('Künstliche Messdaten');\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c4380",
   "metadata": {},
   "source": [
    "### Zusammenfassung\n",
    "\n",
    "In diesem Abschnitt haben wir uns mit nichtlinearen Support Vector Machines\n",
    "beschäftigt. Die Idee zur Klassifizierung nichtlinearer Daten ist, ein neues\n",
    "Feature hinzuzufügen. Mathematisch gesehen projizieren wir also die Daten mit\n",
    "einer nichtlinearen Transformationsfunktion in einen höherdimensionalen Raum und\n",
    "trennen sie in dem höherdimensionalen Raum. Dnn kehren wir durch den Schnitt der\n",
    "Trennebene mit der Transformationsfunktion wieder in den ursprünglichen Raum\n",
    "zurück. Wenn wir als Transformationsfunktion die sogenannten Kernel-Funktionen\n",
    "verwenden, können wir auf die Transformation der Daten verzichten und die\n",
    "Transformation direkt in die SVM einbauen. Das wird Kernel-Trick genannt und\n",
    "sorgt für die Effizienz und damit Beliebtheit von SVMs."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
