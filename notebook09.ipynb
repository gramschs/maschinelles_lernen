{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b18b9aa",
   "metadata": {},
   "source": [
    "# 9. Ensemble-Methoden (Random Forests und XGBoost)\n",
    "\n",
    "## 9.1 Stacking, Bagging und Boosting\n",
    "\n",
    "Eins, zwei, viele ... im Bereich des maschinellen Lernens sind Ensemble-Methoden\n",
    "eine leistungsstarke Technik zur Verbesserung der Modellgenauigkeit und\n",
    "Robustheit. Diese Methoden kombinieren mehrere Modelle, um die Gesamtleistung zu\n",
    "steigern, indem sie die individuellen Stärken der Modelle nutzen und deren\n",
    "Schwächen ausgleichen. In diesem Kapitel werden wir die grundlegenden\n",
    "Konzepte und Unterschiede zwischen diesen drei Methoden erläutern, um ein\n",
    "besseres Verständnis ihrer Funktionsweise und Anwendungen zu vermitteln.\n",
    "\n",
    "### Lernziele Kapitel 9.1\n",
    "\n",
    "* Sie können in eigenen Worten erklären, was **Ensemble-Methoden** sind.\n",
    "* Sie können die drei Ensemble-Methoden\n",
    "  * **Stacking**,\n",
    "  * **Bagging** und\n",
    "  * **Boosting**\n",
    "\n",
    "  mit Hilfe einer Skizze erklären.\n",
    "\n",
    "### Ensemble-Methoden\n",
    "\n",
    "Der Begriff »Ensemble« wird im Allgemeinen eher mit Musik und Kunst in\n",
    "Verbindung gebracht als mit Informatik. In der Musik bezeichnet ein Ensemble\n",
    "eine kleine Gruppe von Musikern, die entweder das gleiche Instrument spielen\n",
    "oder verschiedene Instrumente kombinieren. Im Theater bezeichnet man eine Gruppe\n",
    "von Schauspielern ebenfalls als Ensemble, und in der Architektur beschreibt der\n",
    "Begriff eine Gruppe von Gebäuden, die in einem besonderen Zusammenhang\n",
    "zueinander stehen.\n",
    "\n",
    "Auch im Bereich des maschinellen Lernens hat sich der Begriff Ensemble\n",
    "etabliert. Mit **Ensemble-Methoden** (Ensemble Learning) wird eine Gruppe von\n",
    "maschinellen Modellen bezeichnet, die zusammen eine Prognose treffen sollen.\n",
    "Ähnlich wie bei Musik-Ensembles können beim **Ensemble Learning** entweder\n",
    "identische Modelle oder verschiedene Modelle kombiniert werden. Diese Modelle\n",
    "können entweder gleichzeitig eine Prognose treffen, die dann kombiniert wird,\n",
    "oder nacheinander verwendet werden, wobei ein Modell auf den Ergebnissen eines\n",
    "anderen aufbaut. Je nach Vorgehensweise unterscheidet man im maschinellen Lernen\n",
    "zwischen **Stacking**, **Bagging** und **Boosting**.\n",
    "\n",
    "In dieser Vorlesung konzentrieren wir uns auf Bagging und Boosting mit ihren\n",
    "bekanntesten Vertretern, den Random Forests und XGBoost. Das Konzept des\n",
    "Stackings wird hier nur kurz ohne weitere Details vorgestellt. Eine allgemeine\n",
    "Einführung in Ensemble-Methoden mit Scikit-Learn findet sich in der\n",
    "[Dokumentation Scikit-Learn →\n",
    "Ensemble](https://scikit-learn.org/stable/modules/ensemble.html).\n",
    "\n",
    "![Stacking](https://gramschs.github.io/book_ml4ing/_images/concept_stacking.svg)\n",
    "\n",
    "Stacking bedeutet auf Deutsch »Stapeln«, es werden sozusagen verschiedene\n",
    "ML-Modelle gestapelt. In einem ersten Schritt werden mehrere ML-Modelle\n",
    "unabhängig voneinander auf den Trainingsdaten trainiert. Jedes dieser Modelle\n",
    "liefert eine Prognose, die dann auf verschiedene Arten miteinander kombiniert\n",
    "werden können. Bei Klassifikationsaufgaben ist **Voting**, also die Wahl durch\n",
    "**Mehrheitsentscheidung**, eine beliebte Methode, um die Einzelprognosen zu\n",
    "kombinieren. Wurden beispielsweie für das Stacking drei ML-Modellen gewählt, die\n",
    "jeweils ja oder nein prognostizieren, dann wird für die finale Prognose das\n",
    "Ergebnis genommen, das die Mehrheit der einzelnen Modelle vorausgesagt hat.\n",
    "Scikit-Learn bietet dafür einen Voting Classifier an, siehe [Dokumentation\n",
    "Scikit-Learn → Voting\n",
    "Classifier](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier).\n",
    "\n",
    "Bei Regressionsaufgaben werden die einzelnen Prognosen häufig gemittelt. Dabei\n",
    "kann entweder der übliche arithmetische Mittelwert verwendet werden oder ein\n",
    "**gewichteter Mittelwert**, was als  **Weighted Averaging** bezeichnet wird.\n",
    "Nichtsdestotrotz wird die Mittelwertbildung bei Regressionsaufgaben von\n",
    "Scikit-Learn ebenfalls als Voting bezeichnet, siehe [Dokumentation Scikit-Learn\n",
    "→ Voting\n",
    "Regressor](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor).\n",
    "\n",
    "Eine alternative Kombinationsmethode ist die Verwendung eines weiteren\n",
    "ML-Modells. In diesem Fall werden die Modelle, die die einzelnen Prognosen\n",
    "liefern, als Basismodelle bezeichnet. In der ML-Community ist auch der\n",
    "Fachbegriff **Weak Learner**, also schwache Lerner, für diese Basismodelle\n",
    "gebräuchlich. Die Prognosen der Basismodelle dienen dann als Trainingsdaten für\n",
    "ein weiteres ML-Modell, das als **Meta-Modell** bezeichnet wird. Diese\n",
    "Ensemble-Methode wird **Stacking** genannt. Weitere Informationen liefert die\n",
    "[Scikit-Learn Dokumentation → Stacked\n",
    "Generalization](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization).\n",
    "\n",
    "Stacking bietet viele Vorteile. Der wichtigste Vorteil ist, dass die\n",
    "Prognosefähigkeit des Gesamtmodells in der Regel deutlich besser ist als die der\n",
    "einzelnen Basismodelle. Die Stärken der Basismodelle werden kombiniert und die\n",
    "Schwächen ausgeglichen. Allerdings erfordert Stacking sehr viel Feinarbeit. Auch\n",
    "steigt die Trainingszeit für das Gesamtmodell, selbst wenn die Basismodelle bei\n",
    "genügend Rechenleistung parallel trainiert werden können. Aus diesem Grund\n",
    "werden wir in dieser Vorlesung kein Stacking verwenden.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "![Bagging](https://gramschs.github.io/book_ml4ing/_images/concept_bagging.svg)\n",
    "\n",
    "Bagging ist eine Ensemble-Methode, ähnlich wie Stacking. Im Gegensatz zum\n",
    "Stacking wird beim Bagging jedoch dasselbe Modell für die Einzelprognosen\n",
    "verwendet. Die Unterschiede in den Einzelprognosen entstehen dadurch, dass für\n",
    "das Training der einzelnen Modelle unterschiedliche Daten verwendet werden.\n",
    "\n",
    "Im ersten Schritt werden zufällige Datenpunkte aus den Trainingsdaten ausgewählt\n",
    "und in einen neuen Datensatz, „Stichprobe 1“, aufgenommen. Nachdem ein\n",
    "Datenpunkt ausgewählt wurde, kehrt er in die ursprüngliche Menge der\n",
    "Trainingsdaten zurück und kann erneut ausgewählt werden. Dieser Prozess wird in\n",
    "der Mathematik als **Ziehen mit Zurücklegen** bezeichnet, auf Englisch\n",
    "**Bootstrapping**. Durch Bootstrapping werden dann noch weitere Stichproben\n",
    "gebildet.\n",
    "\n",
    "Im zweiten Schritt wird ein ML-Modell gewählt und für jede Bootstrap-Stichprobe\n",
    "trainiert. Da die Stichproben unterschiedliche Trainingsdaten enthalten,\n",
    "entstehen unterschiedlich trainierte Modelle, die für neue Daten verschiedene\n",
    "Einzelprognosen liefern. Diese Einzelprognosen werden kombiniert bzw. nach\n",
    "festgelegten Regeln zu einer finalen Prognose zusammengefasst. In der Statistik\n",
    "wird die Zusammenfassung von Daten als Aggregation bezeichnet. Auf Englisch\n",
    "heißt der Vorgang des Zusammenfassens **Aggregating**.\n",
    "\n",
    "Die beiden wesentlichen Schritte der Bagging-Methode sind also **B**ootstrapping\n",
    "und **Agg**regat**ing**, was zu der Abkürzung Bagging geführt hat. Scikit-Learn\n",
    "bietet sowohl für Klassifikations- als auch für Regressionsaufgaben eine\n",
    "allgemeine Implementierung der Bagging-Methode an (siehe [Dokumentation\n",
    "Scikit-Learn →\n",
    "Bagging](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator)).\n",
    "Die bekannteste Bagging-Methode ist das Verfahren **Random Forests**, bei dem\n",
    "Entscheidungsbäume (Decision Trees) auf unterschiedlichen Stichproben trainiert\n",
    "und aggregiert werden. Random Forests werden wir im nächsten Kapitel\n",
    "detaillierter betrachten. Vorab beschäftigen wir uns noch mit dem Konzept der\n",
    "Boosting-Methoden.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "![Boosting](https://gramschs.github.io/book_ml4ing/_images/concept_boosting.svg)\n",
    "\n",
    "Das englische Verb „to boost sth.“ hat viele Bedeutungen. Insbesondere wird es\n",
    "im Deutschen mit „etwas verstärken“ übersetzt. Im Kontext des maschinellen\n",
    "Lernens bezeichnet **Boosting** eine Ensemble-Methode, bei der mehrere ML-Modelle\n",
    "hintereinander geschaltet werden, um die Genauigkeit der Prognose zu verstärken.\n",
    "Die Idee des Boosting besteht darin, dass jedes Modell die Fehler des\n",
    "Vorgängermodells reduziert. Es gibt mehrere Varianten zur Fehlerreduktion, aus\n",
    "denen sich unterschiedliche Boosting-Methoden ableiten. Die wichtigsten\n",
    "Varianten sind:\n",
    "\n",
    "* Adaboost,\n",
    "* Gradient Boosting und\n",
    "* Stochastic Gradient Boosting.\n",
    "\n",
    "Beim **Adaboost**-Verfahren wird im ersten Schritt ein Modell (z.B. ein\n",
    "Entscheidungsbaum) auf den Trainingsdaten trainiert. Anschließend werden die\n",
    "Prognosen dieses Modells mit den tatsächlichen Werten verglichen. Im zweiten\n",
    "Schritt wird ein neuer Datensatz erstellt, wobei die Datenpunkte, die falsch\n",
    "prognostiziert wurden, ein größeres Gewicht erhalten. Nun wird erneut ein Modell\n",
    "trainiert; und dessen Prognosen werden wieder mit den echten Werten verglichen.\n",
    "Dieser Vorgang wird mehrfach wiederholt. Das Training der Modelle erfolgt\n",
    "sequentiell, da jedes Vorgängermodell die neue Gewichtung der Trainingsdaten\n",
    "liefert. Am Ende werden alle Einzelprognosen gewichtet zu einer finalen Prognose\n",
    "kombiniert. Weitere Details finden sich in der [Dokumentation Scikit-Learn →\n",
    "Adaboost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost).\n",
    "\n",
    "Beim **Gradient Boosting** wird ebenfalls ein sequentieller Ansatz verfolgt,\n",
    "aber der Fokus liegt auf der Minimierung der Fehler. Im ersten Schritt wird ein\n",
    "ML-Modell (häufig ein Entscheidungsbaum) trainiert. Danach wird für jeden\n",
    "Datenpunkt der Fehler des Modells, das sogenannte **Residuum**, berechnet, indem\n",
    "die Differenzen zwischen dem tatsächlichen Wert und der Prognosen bestimmt wird.\n",
    "Im nächsten Schritt wird ein neues Modell trainiert, das darauf abzielt, diese\n",
    "Residuen vorherzusagen. Dieses neue Modell wird dann zu dem vorherigen Modell\n",
    "hinzugefügt, um die Gesamtprognose zu verbessern. Dieser Prozess wird\n",
    "wiederholt, wobei in jeder Iteration ein neues Modell trainiert wird, das die\n",
    "Fehler der bisherigen Modelle reduziert (mit Hilfe einer Verlustfunktion und\n",
    "eines Gradientenverfahrens). Am Ende ergibt sich eine starke Vorhersage, indem\n",
    "alle Modelle kombiniert werden. Da sehr häufig Entscheidungsbäume als Modell\n",
    "gewählt werden, bietet Scikit-Learn eine Implementierung der sogenannten\n",
    "**Gradient Boosted Decision Trees** an, siehe [Dokumentation Scikit-Learn →\n",
    "Gradient-boosted\n",
    "trees](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees).\n",
    "\n",
    "**Stochastic Gradient Boosting** ist eine Erweiterung des Gradient Boosting, bei\n",
    "der zusätzlich Stochastik eingeführt wird. Hierbei wird in jedem Schritt nur\n",
    "eine zufällige Stichprobe der Trainingsdaten verwendet, um ein Modell zu\n",
    "trainieren. Der Trainingsprozess ähnelt dem von Gradient Boosting, wobei in\n",
    "jeder Runde ein neues Modell trainiert wird, das die Fehler der vorherigen\n",
    "Modelle korrigiert. Durch die zufällige Auswahl der Trainingsdaten in jeder\n",
    "Iteration wird eine höhere Robustheit gegenüber Overfitting (Überanpassung)\n",
    "erreicht. Stochastic Gradient Boosting wird nicht direkt von Scikit-Learn\n",
    "unterstützt. Eine sehr bekannte Implmentierung davon ist XGBoost (siehe\n",
    "[https://xgboost.readthedocs.io/](https://xgboost.readthedocs.io/en/stable/)),\n",
    "die wir in einem der nächsten Kapitel noch näher betrachten werden.\n",
    "\n",
    "### Zusammenfassung und Ausblick Kapitel 9.1\n",
    "\n",
    "In diesem Kapitel haben Sie die drei Konzepte Stacking, Bagging und Boosting\n",
    "eher theoretisch kennengelernt. Alle drei Methoden sind Ensemble-Methoden, bei\n",
    "denen mehrere ML-Modelle parallel oder sequentiell kombiniert werden. Obwohl\n",
    "diese Ensemble-Methoden allgemein für verschiedene ML-Modelle eingesetzt werden\n",
    "können, haben sich vor allem Random Forests (Bagging für Entscheidungsbäume) und\n",
    "Stochastic Gradient Boosting als besonders effektiv erwiesen. Letztere sind\n",
    "nicht in Scikit-Learn implementiert, sondern werden durch eine eigene Bibliothek\n",
    "namens XGBoost bereitgestellt. In den nächsten beiden Kapiteln werden wir beide\n",
    "auch mit praktischen Beispielen vertiefen.\n",
    "\n",
    "## 9.2 Random Forests\n",
    "\n",
    "Im letzten Kapitel haben wir verschiedene Ensemble-Methoden in der Theorie\n",
    "kennengelernt: Stacking, Bagging und Boosting. Für die beiden letzteren\n",
    "Ensemble-Methoden werden besonders häufig Entscheidungsbäume (Decision Trees)\n",
    "eingesetzt. Daher betrachten wir in diesem Kapitel die praktische Umsetzung von\n",
    "Bagging mit Entscheidungsbäumen, die sogenannten Random Forests.\n",
    "\n",
    "### Lernziele Kapitel 9.2\n",
    "\n",
    "* Sie können das ML-Modell **Random Forest** in der Praxis anwenden.\n",
    "* Sie können mit Hilfe der **Feature Importance** bewerten, wie groß der\n",
    "  Einfluss eines Merkmals auf die Prognosegenauigkeit des Random Forests ist.\n",
    "\n",
    "### Random Forests mit Scikit-Learn\n",
    "\n",
    "Entscheidungsbäume (Decision Trees) haben wir bereits betrachtet. Sie sind\n",
    "aufgrund ihrer Einfachheit und vor allem aufgrund ihrer Interpretierbarkeit sehr\n",
    "beliebt. Allerdings ist ihre Tendenz zum Overfitting problematisch. Daher\n",
    "kombinieren wir die Ensemble-Methode Bagging mit Entscheidungsbäumen (Decision\n",
    "Trees). Indem aus den Trainingsdaten zufällig kleinere Bootstrap-Stichproben\n",
    "ausgewählt werden, erhalten wir unterschiedliche Entscheidungsbäume (Decision\n",
    "Trees). Zusätzlich wird beim Training der Entscheidungsbäume nicht mit allen\n",
    "Merkmalen (Features) trainiert, sondern auch hier wählen wir die Merkmale\n",
    "zufällig aus. Durch diese zwei Maßnahmen wird die Anpassung der\n",
    "Entscheidungsbäume an die Trainingsdaten (Overfitting) reduziert.\n",
    "\n",
    "Um den Random Forest von Scikit-Learn praktisch auszuprobieren, erzeugen wir\n",
    "künstliche Daten. Dazu verwenden wir die Funktion `make_moons` von Scikit-Learn,\n",
    "die Zufallszahlen generiert und interpretieren die Zufallszahlen als\n",
    "Kilometerstände und Preise von Autos bei einer fiktiven Verkaufsaktion.\n",
    "Zusätzlich lassen wir zufällig Nullen und Einsen erzeugen, die wir als\n",
    "»verkauft« oder »nicht verkauft« interpretieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Erzeugung künstlicher Daten\n",
    "X_array, y_array = make_moons(n_samples=120, random_state=0, noise=0.3)\n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n",
    "    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n",
    "    'verkauft': y_array,\n",
    "    })\n",
    "\n",
    "# Adaption der Daten\n",
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\n",
    "y = daten['verkauft'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37096d",
   "metadata": {},
   "source": [
    "Diesmal werden in dem Autohaus 120 Autos zum Verkauf angeboten (siehe Option\n",
    "`n_samples=120`). Nach Aktionsende werden die Merkmale Kilometerstand und Preis\n",
    "tabellarisch erfasst und notiert, ob das Auto verkauft wurde (True bzw. 1) oder\n",
    "nicht verkauft wurde (False bzw. 0). Wir visualisieren die Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# plot artificial data\n",
    "fig = px.scatter(daten, x = 'Kilometerstand [km]', y = 'Preis [EUR]', color=daten['verkauft'].astype(bool),\n",
    "        title='Künstliche Daten: Verkaufsaktion Autohaus',\n",
    "        labels = {'x': 'Kilometerstand [km]', 'y': 'Preis [EUR]', 'color': 'verkauft'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1451a12c",
   "metadata": {},
   "source": [
    "Nachdem wir die Vorbereitungen für die Daten abgeschlossen haben, können wir\n",
    "Scikit-Learn einen Random Forest trainieren lassen. Dazu importieren wir den\n",
    "Algorithmus aus dem Modul `ensemble`. Da der Random Forest ein Ensemble von\n",
    "Entscheidungsbäumen (Decision Trees) ist, haben wir nun die Möglichkeit, die\n",
    "Anzahl der Entscheidungsbäume festzulegen. Voreingestellt sind 100\n",
    "Entscheidungsbäume. Aus didaktischen Gründen reduzieren wir diese Anzahl auf\n",
    "vier und setzen das Argument `n_estimators=` auf `4`. Ebenfalls aus didaktischen\n",
    "Gründen fixieren wir die Zufallszahlen, mit Hilfer derer das Bootstrapping und\n",
    "die Auswahl der Merkmale (Features) umgesetzt wird, mit `random_state=0`. In\n",
    "einem echten Projekt würden wir das unterlassen. Zuletzt führen wir das Training\n",
    "mit der `.fit()`-Methode durch. Weitere Details finden Sie unter [Scikit-Learn\n",
    "Dokumentation →\n",
    "RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_random_forest = RandomForestClassifier(n_estimators=4, random_state=0)\n",
    "model_random_forest.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8efa57",
   "metadata": {},
   "source": [
    "Als nächstes lassen wir den Random Forest für jeden Punkte des Gebiets\n",
    "prognostizieren, ob ein Auto mit diesem Kilometerstand und diese Preis\n",
    "verkaufbar wäre oder nicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "my_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\n",
    "fig = DecisionBoundaryDisplay.from_estimator(model_random_forest, X,  cmap=my_colormap)\n",
    "fig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\n",
    "fig.ax_.set_xlabel('Kilometerstand [km]');\n",
    "fig.ax_.set_ylabel('Preis [EUR]');\n",
    "fig.ax_.set_title('Random Forest: Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805fe76",
   "metadata": {},
   "source": [
    "Möchte man die vier Entscheidungsbäume (Decision Trees) analysieren, aus denen\n",
    "der Random Forest kombiniert wurde, kann man mit dem Attribut `estimators_`\n",
    "darauf zugreifen. Wir lassen uns jetzt die Entscheidungsgrenzen einzeln für\n",
    "jeden Entscheidungsbaum anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\n",
    "for (nummer, baum) in zip(range(4), model_random_forest.estimators_):\n",
    "    fig = DecisionBoundaryDisplay.from_estimator(baum, X,  cmap=my_colormap)\n",
    "    fig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\n",
    "    fig.ax_.set_xlabel('Kilometerstand [km]');\n",
    "    fig.ax_.set_ylabel('Preis [EUR]');\n",
    "    fig.ax_.set_title(f'Entscheidungsbaum {nummer+1}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72563552",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Der Random Forest reduziert das Overfitting und ist damit für zukünftige\n",
    "Prognosen besser gerüstet, verliert aber seine leichte Interpretierbarkeit. Ein\n",
    "großer Vorteil des Entscheidungsbaumes (Decision Trees) ist ja, dass wir die\n",
    "Entscheidungen als eine Abfolge von Entscheidungsfragen gut nachvollziehen\n",
    "können. Jeder der einzelnen Entscheidungsbäume kommt jedoch zu einer anderen\n",
    "Reihenfolge der Entscheidungsfragen und zu anderen Grenzen. Dafür bietet der\n",
    "Random-Forest-Algorithmus eine alternative Bewertung, wie wichtig einzelne\n",
    "Merkmale (Features) sind, die sogenannte **Feature Importance**.\n",
    "\n",
    "Feature Importance bewertet, wie wichtig der Einfluss eines Merkmals (Features)\n",
    "auf die Prognoseleistung ist. Ist die Feature Importance eines Merkmals\n",
    "(Features) höher, so trägt dieses Merkmal (Feature) auch mehr zu der Genauigkeit\n",
    "der Prognose bei. Bei Entscheidungsbäumen wird für jedes Merkmal (Feature)\n",
    "berechnet, wie groß die Reduktion der Gini-Impurity ist. Gibt es ein Merkmal,\n",
    "das eindeutig die Gini-Impurity reduziert, dann hat dieses Merkmal auch einen\n",
    "großen Einfluss auf die Prognosefähigkeit des Modells. Wir könnten nach dem\n",
    "Training des Entscheidungsbaumes zusammenfassen, wie oft und wieviel ein\n",
    "bestimmtes Merkmal zur Reduktion beiträgt. In der Praxis kommt es aber oft vor,\n",
    "dass bei einem Split mehrere Merkmale gleichermaßen die Gini-Impurity\n",
    "reduzieren. Dann wird eines der Merkmale zufällig ausgewählt. Daher kann es\n",
    "schwieirg sein, bei einem Entscheidungsbaum die Feature Importance zu bewerten.\n",
    "Bei einem Random Forest hingegen werden viele Entscheidungsbäume trainiert. Wenn\n",
    "wir jetzt bei allen Entscheiungsbäumen die Feature Importance berechnen und den\n",
    "Mittelwert bilden, erhalten wir ein aussagekräftiges Bewertungskriterium, wie\n",
    "stark einzelne Merkmale die Prognosefähigkeit beeinflussen.\n",
    "\n",
    "Wir trainieren nun einen Random Forest mit der Standardeinstellung von 100\n",
    "Entscheidungsbäumen und lassen uns dann die Feature Importance ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99271833",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X,y)\n",
    "\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ded35",
   "metadata": {},
   "source": [
    "Der erste Wert gibt die Feature Importance für das erste Merkmal an und der zweite Wert entsprechend für das zweite Merkmal. Es ist üblich, die Feature Importance als Balkendiagramm zu visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(model.feature_importances_, index=['Kilometerstand [km]', 'Preis [EUR]'])\n",
    "\n",
    "fig = px.bar(feature_importances, orientation='h',\n",
    "  title='Verkaufsaktion im Autohaus', \n",
    "  labels={'value':'Feature Importance', 'index': 'Merkmal'})\n",
    "fig.update_traces(showlegend=False) \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4103438",
   "metadata": {},
   "source": [
    "Der Preis ist demnach wichtiger als der Kilometerstand (wobei es hier ja ein\n",
    "künstliches Beispiel ist).\n",
    "\n",
    "### Zusammenfassung und Ausblick 9.2\n",
    "\n",
    "Random Forests sind einfachen Entscheidungsbäumen vorzuziehen, da sie das\n",
    "Overfitting reduzieren. Die Erzeugung der einzelnen Entscheidungsbäume kann\n",
    "parallelisiert werden, so dass das Training eines Random Forests sehr schnell\n",
    "durchgeführt werden kann. Auch für große Datenmengen mit sehr unterschiedlichen\n",
    "Eigenschaften arbeitet der Random Forest sehr effizient. Er ermöglicht auch eine\n",
    "Interpretation, welche Eigenschaften ggf. einen größeren Einfluss haben als\n",
    "andere Eigenschaften\n",
    "\n",
    "## 9.3 XGBoost\n",
    "\n",
    "In der bisherigen Vorlesung haben wir vor allem Pandas und Scikit-Learn benutzt.\n",
    "Zwar bietet Scikit-Learn Boosting-Verfahren an, in vielen Wettbewerben hat sich\n",
    "jedoch eine andere Bibliothek durchgesetzt, die eine optimierte Variante des\n",
    "Stochastic Gradient Boosting anbietet: **XGBoost**.\n",
    "\n",
    "**Warnung:** Falls bei Ihnen XGBoost nicht installiert sein sollte, folgen Sie bitte den Anweisungen auf der Internesetseite [https://xgboost.readthedocs.io](https://xgboost.readthedocs.io/en/stable/install.html) und installieren Sie XGBoost jetzt nach.\n",
    "\n",
    "### Lernziele Kapitel 9.3\n",
    "\n",
    "* Sie können XGBoost für Regressions- und Klassifikationsaufgaben einsetzen.\n",
    "* Sie wissen, wie Sie mit Analysen der Maßzahlen Fehler und Logloss für\n",
    "  Trainings- und Testdaten beurteilen können, ob Überanpassung (Overfitting)\n",
    "  vorliegt.\n",
    "* Sie kennen die Methode **Frühes Stoppen** zur Reduzierung der Überanpassung\n",
    "  (Overfitting).\n",
    "* Sie wissen, dass XGBoost nicht manuell feinjustiert werden sollte, sondern mit\n",
    "  Gittersuche oder weiteren Bibliotheken (z.B. Optuna).\n",
    "\n",
    "### XGBoost benutzt Scikit-Learn API\n",
    "\n",
    "XGBoost steht für e**X**treme **G**radient **Boost**ing und ist aus\n",
    "Performancegründen in der Programmiersprache C++ implementiert. Für\n",
    "Python-Programmier wurde ein Python-Modul mit dem Ziel geschaffen, die gleichen\n",
    "Schnittstellen wie Scikit-Learn anzubieten, so dass kaum Einarbeitungszeit in\n",
    "eine neue Bibliothek erforderlich ist. Vor allem benötigen Data Scientists auch\n",
    "keine C++\\-Programmierkenntnisse, sondern können weiterhin mit Python arbeiten.\n",
    "\n",
    "Wir bleiben bei unserem Beispiel mit der Verkaufsaktion im Autohaus aus dem\n",
    "vorherigen Kapitel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96257d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Erzeugung künstlicher Daten\n",
    "X_array, y_array = make_moons(n_samples=120, random_state=0, noise=0.3)\n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n",
    "    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n",
    "    'verkauft': y_array,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fb4d4",
   "metadata": {},
   "source": [
    "XGBoost kann Pandas DataFrames nicht verarbeiten, sondern benötigt die reinen\n",
    "Zahlenwerte in Form von Matrizen. Das ist in der Tat kein Problem, denn die\n",
    "Datenstruktur DataFrame stellt die reinen Matrizen über die Methode `.values`\n",
    "direkt zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19294277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaption der Daten\n",
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\n",
    "y = daten['verkauft'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefebf8",
   "metadata": {},
   "source": [
    "Als nächstes importieren wir XGBoost. Es ist üblich, das ganze Modul zu\n",
    "importieren und mit `xgb` abzukürzen. Danach initialisieren wir das\n",
    "Klassifikationsmodell `XGBClassifier` und trainieren es auf den Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb \n",
    "\n",
    "modell = xgb.XGBClassifier()\n",
    "modell.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6837ce",
   "metadata": {},
   "source": [
    "Als nächstes visualisieren wir die Prognose des trainierten\n",
    "XGBoost-Klassifikators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "my_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\n",
    "fig = DecisionBoundaryDisplay.from_estimator(modell, X,  cmap=my_colormap)\n",
    "fig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\n",
    "fig.ax_.set_xlabel('Kilometerstand [km]');\n",
    "fig.ax_.set_ylabel('Preis [EUR]');\n",
    "fig.ax_.set_title('XGBoost: Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb194fe",
   "metadata": {},
   "source": [
    "Die Entscheidungsgrenzen sehr plausibel aus.\n",
    "\n",
    "### XGBoost neigt stark zur Überanpassung (Overfitting)\n",
    "\n",
    "XGBoost ist bekannt für Überanpassung (Overfitting) an die Trainingsdaten. Um\n",
    "das an unserem Beispiel mit der Verkaufsaktion im Autohaus zu zeigen, fügen wir\n",
    "noch neue, unbekannte Testdaten hinzu. Dazu verdoppeln wir die Anzahl der Autos\n",
    "(`n_samples=2000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugung künstlicher Daten\n",
    "X_array, y_array = make_moons(n_samples=2000, random_state=0, noise=0.3)\n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n",
    "    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n",
    "    'verkauft': y_array,\n",
    "    })\n",
    "\n",
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\n",
    "y = daten['verkauft'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab89156",
   "metadata": {},
   "source": [
    "Anschließend teilen wir die 2000 Autos in zwei Gruppen: Trainings- und\n",
    "Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acda88",
   "metadata": {},
   "source": [
    "Diesmal legen wir explizit fest, aus wievielen Modellen das Boosting-Verfahren\n",
    "bestehen soll. Dazu setzen wir `n_estimators=200`. Oft wird auch von der Anzahl\n",
    "der »Boosting-Runden« gesprochen. Das Training auf den Trainingsdaten liefert\n",
    "sehr gute Ergebnisse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "modell = xgb.XGBClassifier(n_estimators=200)\n",
    "\n",
    "modell.fit(X_train, y_train)\n",
    "\n",
    "score_train = modell.score(X_train, y_train)\n",
    "print(f'Score bezogen auf Trainingsdaten: {score_train:.2f}')\n",
    "score_test = modell.score(X_test, y_test)\n",
    "print(f'Score bezogen auf Testdaten: {score_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da982f0",
   "metadata": {},
   "source": [
    "Die Trainingsdaten werden perfekt prognostiziert. Auch bei den Testdaten\n",
    "erhalten wir ein gutes Ergebnis, das aber im Vergleich zu dem sehr guten Score\n",
    "bei den Trainingsdaten abfällt. Es fällt schwer, zu entscheiden, ob eine\n",
    "Überanpassung (Overfitting) vorliegt. XGBoost ist ein iteratives Verfahren.\n",
    "Zunächst wird Modell Nr. 1 trainiert, darauf aufbauend Modell Nr. 2 usw. Wir\n",
    "wiederholen jetzt das Training des XGBoost-Klassifikators, aber lassen durch ein\n",
    "weiteres Argument mitprotokollieren, was in den einzelnen Iterationen passiert.\n",
    "\n",
    "Zuerst legen wir fest, welche internen Bewertungskennzahlen (= Metrik, Maßzahl)\n",
    "mitprotokolliert werden sollen. Wir wählen als erste Maßzahl den Fehler, also\n",
    "die relative Anzahl der falsch klassifizierten Autos. Die zweite Maßzahl\n",
    "berechnet, wie weit die Wahrscheinlichkeit für »verkauft« oder »nicht verkauft«\n",
    "vom tatsächlichen Ergebnis weg ist. Mathmatisch etwas präziser betrachten wir\n",
    "die [Kreuzentropie](https://de.wikipedia.org/wiki/Kreuzentropie), bekannt als\n",
    "»Losslog«.\n",
    "\n",
    "Technisch setzen wir dies um, indem wir bei der Initialisierung des\n",
    "XGBoost-Modells das optionale Argument `eval_metric=['error', 'logloss']`\n",
    "setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = xgb.XGBClassifier(n_estimators=200, eval_metric=['error', 'logloss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d680bb42",
   "metadata": {},
   "source": [
    "Allerdings ist damit noch nicht festgelegt, auf welchen Daten die Fehler-Maßzahl\n",
    "und die Logloss-Maßzahl berechnet werden. Zunächst sollen beide Maßzahlen für\n",
    "die Trainingsdaten berechnet werden, dann für die Testdaten. Das erreichen wir\n",
    "mit dem optionalen Argument `eval_set=`, dem wir folgendermaßen die Trainings-\n",
    "und Testdaten mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a536770",
   "metadata": {},
   "source": [
    "Wir setzen noch `verbose=False`, damit nicht für jedes Modell bzw. jede\n",
    "Iteration die vier Maßzahlen auf dem Bildschirm ausgegeben werden. Nach dem\n",
    "Training können wir die vier Maßzahlen mit der Methode `.evals_result()` aus dem\n",
    "trainierten Modell extrahieren. Um die Maßzahlen zu visualisieren, packen wir\n",
    "sie in einen Pandas-DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f653fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "masszahlen = modell.evals_result()\n",
    "fehler = pd.DataFrame({\n",
    "    'Fehler Trainingsdaten': masszahlen['validation_0']['error'],\n",
    "    'Fehler Testdaten': masszahlen['validation_1']['error']\n",
    "    })\n",
    "losslog = pd.DataFrame({\n",
    "    'Losslog Trainingsdaten': masszahlen['validation_0']['logloss'],\n",
    "    'Losslog Testdaten': masszahlen['validation_1']['logloss']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd297e3",
   "metadata": {},
   "source": [
    "Wir visualisieren Fehler und Losslog getrennt voneinander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ff92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "\n",
    "fig = px.scatter(fehler,\n",
    "    title='Fehler in jeder Iteration (Boosting-Runde)',\n",
    "    labels={'value': 'Fehler', 'index': 'Iteration', 'variable': 'Legende'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14167d67",
   "metadata": {},
   "source": [
    "Der Fehler bei den Trainingsdaten wird von Boosting-Runde zu Boosting-Runde\n",
    "kleiner, aber der Fehler der Testdaten wächst. Zunächst wird der Fehler der\n",
    "Testdaten kleiner, erreicht in Minimum in der 6. Iteration, um dann wieder zu\n",
    "steigen. Dieses Verhalten ist typisch für Überanpassung (Overfitting). Etwas\n",
    "deutlicher wird dieses Phänomen, wenn wir uns die (transoformierte) Differenz\n",
    "der Wahrscheinlichkeiten ansehen, die Losslog-Maßzahl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba247d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "\n",
    "fig = px.scatter(losslog,\n",
    "    title='Losslog in jeder Iteration (Boosting-Runde)',\n",
    "    labels={'value': 'Losslog', 'index': 'Iteration', 'variable': 'Legende'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14056df6",
   "metadata": {},
   "source": [
    "Am kleinsten ist die Losslog-Maßzahl für die Iteration 9, danach steigt die\n",
    "Losslog-Maßzahl wieder an. Am besten wäre es nach dieser Analyse gewesen, nach\n",
    "der 6. oder 9. Iteration aufzuhören, da dann die Überanpassung (Overfitting) an\n",
    "die Trainingsdaten einsetzt.\n",
    "\n",
    "### Bekämpfen von Überanpassung (Overfitting)\n",
    "\n",
    "Es gibt einige Hyperparamter von XGBoost, die helfen, Überanpassung\n",
    "(Overfitting) zu reduzieren. Eine Möglichkeit ist es, früher zu stoppen und\n",
    "nicht die voreingestellte Anzahl an Modellen bzw. Iterationen / Boosting-Runden\n",
    "zu durchlaufen. Das wird durch das optionale Argument `early_stopping_rounds=`\n",
    "im Konstruktor ermöglicht. Die Zahl, die diesem Parameter übergeben wird, gibt\n",
    "die Anzahl der Boosting-Runden vor, nach denen gestoppt wird, falls sich kaum\n",
    "etwas an der Maßzahl geändert hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a121ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = xgb.XGBClassifier(n_estimators=200, early_stopping_rounds=10, eval_metric=['error', 'logloss'])\n",
    "modell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b632e",
   "metadata": {},
   "source": [
    "Visualisiert sieht die Losslog-Statistik für das obige Beispiel so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masszahlen = modell.evals_result()\n",
    "fehler = pd.DataFrame({\n",
    "    'Fehler Trainingsdaten': masszahlen['validation_0']['error'],\n",
    "    'Fehler Testdaten': masszahlen['validation_1']['error']\n",
    "    })\n",
    "losslog = pd.DataFrame({\n",
    "    'Losslog Trainingsdaten': masszahlen['validation_0']['logloss'],\n",
    "    'Losslog Testdaten': masszahlen['validation_1']['logloss']\n",
    "    })\n",
    "\n",
    "fig = px.scatter(fehler,\n",
    "    title='Frühes Stoppen: Fehler',\n",
    "    labels={'value': 'Fehler', 'index': 'Iteration', 'variable': 'Legende'})\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(losslog,\n",
    "    title='Frühes Stoppen: Losslog',\n",
    "    labels={'value': 'Losslog', 'index': 'Iteration', 'variable': 'Legende'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319bbcd6",
   "metadata": {},
   "source": [
    "Eine weitere Möglichkeit, Überanpassung (Overfitting) zu reduzieren, besteht\n",
    "darin, die Tiefe der Entscheidungsbäume zu begrenzen. Wir benutzen\n",
    "Entscheidungsbaum-Stümpfe, die eine Tiefe von Eins haben. Das erreichen wir mit\n",
    "dem optionalen Argument `max_depth=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = xgb.XGBClassifier(max_depth=1, n_estimators=200, eval_metric=['error', 'logloss'])\n",
    "modell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n",
    "\n",
    "masszahlen = modell.evals_result()\n",
    "fehler = pd.DataFrame({\n",
    "    'Fehler Trainingsdaten': masszahlen['validation_0']['error'],\n",
    "    'Fehler Testdaten': masszahlen['validation_1']['error']\n",
    "    })\n",
    "losslog = pd.DataFrame({\n",
    "    'Losslog Trainingsdaten': masszahlen['validation_0']['logloss'],\n",
    "    'Losslog Testdaten': masszahlen['validation_1']['logloss']\n",
    "    })\n",
    "\n",
    "fig = px.scatter(fehler,\n",
    "    title='Begrenzte Entscheidungsbäume: Fehler',\n",
    "    labels={'value': 'Fehler', 'index': 'Iteration', 'variable': 'Legende'})\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(losslog,\n",
    "    title='Begrenzte Entscheidungsbäume: Losslog',\n",
    "    labels={'value': 'Losslog', 'index': 'Iteration', 'variable': 'Legende'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336d3a9",
   "metadata": {},
   "source": [
    "Es gibt noch einige weitere Hyperparameter, die für \"das\" beste Modell\n",
    "feinjustiert werden können. Händisch gelingt es kaum, alle Hyperparameter\n",
    "optimal einzustellen, so dass hier eine Gittersuche oder gar eine Bibliothek wie\n",
    "[Optuna](https://github.com/optuna/optuna) eingesetzt werden sollte.\n",
    "\n",
    "### Zusammenfassung und Ausblick Kapitel 9.3\n",
    "\n",
    "Mit XGBoost haben Sie ein ML-Modell für das überwachte Lernen kennengelernt, das\n",
    "in den vergangen Jahren sehr viele Wettbewerbe gewonnen hat. Die Mächtigkeit der\n",
    "Algorithmen führt aber häufig zur Überanpassung (Overfitting), so dass die\n",
    "sorgsame Feinjustierung der Hyperparameter besonders wichtig ist.\n",
    "\n",
    "## Übungen\n",
    "\n",
    "Beschäftigen Sie sich zum Abschluss des Jahres 2024 mit Ihren eigenen\n",
    "Kontodaten. Laden Sie sich von Ihrer Bank Ihre Kontoauszüge als Excel- oder\n",
    "csv-Datei herunter und importieren Sie sie. Behandeln Sie dann Ihre Ein- und\n",
    "Ausgaben wie ein ML-Projekt. Sind die Daten vollständig? Was sind numerische\n",
    "Eigenschaften, was kategoriale Merkmale? Führen Sie eine explorative\n",
    "Datenanalyse durch.\n",
    "\n",
    "Recherchieren Sie im Internet, Was sind passende Kategorien für ein\n",
    "Haushaltsbuch? Nutzen Sie die replace()-Methode, um Python automatisch die\n",
    "Einträge in Kategorien sortieren zu lassen (führen Sie die Kategorisierung ggf.\n",
    "nur für einen Monat oder ein Quartal durch). Welches ML-Modell ist am besten\n",
    "geeignet, um eine Prognose Ihres Kontostandes für 2025 zu erstellen?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
