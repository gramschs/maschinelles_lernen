{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75ae6a0",
   "metadata": {},
   "source": [
    "# 6. Entscheidungsbäume (Decision Trees)\n",
    "\n",
    "## 6.1 Was ist ein Entscheidungsbaum?\n",
    "\n",
    "Ein beliebtes Partyspiel ist das Spiel \"Wer bin ich?\". Die Spielregel sind\n",
    "simpel. Eine Person wählt eine berühmte Person oder eine Figur aus einem Film\n",
    "oder Buch, die die Mitspieler:innen erraten müssen. Die Mitspieler:innen\n",
    "dürfen jedoch nur Fragen stellen, die mit \"Ja\" oder \"Nein\" beantwortet werden.\n",
    "\n",
    "Hier ist ein Beispiel, wie eine typische Runde von \"Wer bin ich?\" ablaufen\n",
    "könnte:\n",
    "\n",
    "* Spieler 1: Bin ich männlich?\n",
    "* Spieler 2: Ja.\n",
    "* Spieler 3: Bist du ein Schauspieler?\n",
    "* Spieler 1: Nein.\n",
    "* Spieler 4: Bist du ein Musiker?\n",
    "* Spieler 1: Ja.\n",
    "* Spieler 5: Bist du Michael Jackson?\n",
    "* Spieler 1: Ja! Richtig!\n",
    "\n",
    "Als nächstes wäre jetzt Spieler 5 an der Reihe, sich eine Person oder Figur\n",
    "auszuwählen, die die anderen Spieler erraten sollen. Vielleicht kennen Sie auch\n",
    "die umgekehrte Variante. Der Name der zu ratenden Person/Figur wird der Person\n",
    "mit einem Zettel auf die Stirn geklebt. Und nun muss die Person raten, während\n",
    "die Mitspieler:innen mit Ja/Nein antworten.\n",
    "\n",
    "Dieser Partyklassiker lässt sich auch auf das maschinelle Lernen übertragen.\n",
    "\n",
    "### Lernziele Kapitel 6.1\n",
    "\n",
    "* Sie wissen, was ein **Entscheidungsbaum (Decision Tree)** ist.\n",
    "* Sie kennen die Bestandteile eines Entscheidungsbaumes:\n",
    "  * Wurzelknoten (Root Node)\n",
    "  * Knoten (Node)\n",
    "  * Zweig oder Kante (Branch)\n",
    "  * Blatt (Leaf)\n",
    "* Sie können einen Entscheidungsbaum mit Scikit-Learn trainieren.\n",
    "* Sie können mit Hilfe eines Entscheidungsbaumes Prognosen treffen.\n",
    "\n",
    "### Ein Entscheidungsbaum im Autohaus\n",
    "\n",
    "Ein **Entscheidungsbaum** gehört zu den überwachten Lernverfahren (Supervised\n",
    "Learning). Es ist auch üblich, die englische Bezeichnung **Decision Tree**\n",
    "anstatt des deutschen Begriffes zu nutzen. Ein großer Vorteil von\n",
    "Entscheidungsbäumen ist ihre Flexibilität, denn sie können sowohl für\n",
    "Klassifikations- als auch Regressionsaufgaben eingesetzt werden. Im Folgenden\n",
    "betrachten wir als Beispiel eine Klassifikationsaufgabe. In einem Autohaus\n",
    "vereinbaren zehn Personen eine Probefahrt. In der folgenden Tabelle ist notiert,\n",
    "welchen\n",
    "\n",
    "* `Kilometerstand [in km]` und\n",
    "* `Preis [in EUR]`\n",
    "\n",
    "das jeweilige Auto hat. In der dritten Spalte `verkauft` ist vermerkt, ob das\n",
    "Auto nach der Probefahrt verkauft wurde (`True`) oder nicht (`False`). Diese\n",
    "Information ist die Zielgröße. Die Tabelle mit den Daten lässt sich effizient\n",
    "mit einem Pandas-DataFrame organisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': [32908, 20328, 13285, 17162, 27449, 13715, 32889,  3111, 15607, 18295],\n",
    "    'Preis [EUR]': [15960, 20495, 17227, 17851, 5428, 22772, 13581, 16793, 23253, 11382],\n",
    "    'verkauft': [False, True, False, True, False, True, False, True, True, False],\n",
    "    },\n",
    "    index=['Auto 1', 'Auto 2', 'Auto 3', 'Auto 4', 'Auto 5', 'Auto 6', 'Auto 7', 'Auto 8', 'Auto 9', 'Auto 10'])\n",
    "daten.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ec792",
   "metadata": {},
   "source": [
    "Da in unserem Beispiel von den Autos nur die beiden Eigenschaften\n",
    "`Kilometerstand [km]` und `Preis [EUR]` erfasst wurden, können wir die\n",
    "Datenpunkte anschaulich in einem zweidimensionalen Streudiagramm (Scatterplot)\n",
    "visualisieren. Dabei wird der Kilometerstand auf der x-Achse und der Preis auf\n",
    "der y-Achse abgetragen. Die Zielgröße `verkauft` kennzeichnen wir durch die\n",
    "Farbe. Dabei steht die Farbe Rot für »verkauft« (True) und Blau für »nicht\n",
    "verkauft« (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(daten, x = 'Kilometerstand [km]', y = 'Preis [EUR]', \n",
    "                 color='verkauft', title='Künstliche Daten: Verkaufsaktion im Autohaus')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf37be4",
   "metadata": {},
   "source": [
    "Als nächstes zeigen wir, wie die Autos anhand von Fragen in die beiden Klassen\n",
    "»verkauft« und »nicht verkauft« sortiert werden können. Im Streudiagramm\n",
    "visualisieren wir die Autos mit ihren Eigenschaften `Kilometerstand [km]` und\n",
    "`Preis [EUR]` als Punkte. Dazu passend werden wir schrittweise den\n",
    "Entscheidungsbaum entwickeln. Ein Entscheidungsbaum visualisiert\n",
    "Entscheidungsregeln in Form einer Baumstruktur. Zu Beginn wurde noch keine Frage\n",
    "gestellt und alle Autos befinden sich gemeinsam in einem **Knoten** (Node) des\n",
    "Entscheidungsbaumes, der visuell durch einen rechteckigen Kasten symbolisiert\n",
    "wird. Dieser erste Knoten wird als **Wurzelknoten** (Root Node) bezeichnet, da\n",
    "er die Wurzel des Entscheidungsbaumes darstellt.\n",
    "\n",
    "Dann wird eine erste Frage gestellt. *Ist der Verkaufspreis kleiner oder gleich\n",
    "16376.50 EUR?* Entsprechend dieser Entscheidung werden die Autos in zwei Gruppen\n",
    "aufgeteilt. Wenn ja, wandern die Autos nach links und ansonsten nach rechts. Im\n",
    "Entscheidungsbaum wird diese Aufteilung durch einen **Zweig** (Branch) nach\n",
    "links und einen Zweig nach rechts symbolisiert. Ein alternativer Name für Zweig\n",
    "ist **Kante**. Die Autos »rutschen« die Zweige/Kanten entlang und landen in zwei\n",
    "separaten Knoten. Im Streudiagramm (Scatterplot) entspricht diese Fragestellung\n",
    "dem Vergleich mit einer horizontalen Linie bei y = 16376.5. Da alle Autos mit\n",
    "einem Verkaufspreis kleiner/gleich 16376.5 EUR blau sind, also »nicht verkauft«\n",
    "wurden, wird im Streudiagramm (Scatterplot) alles unterhalb der horizontalen\n",
    "Linie blau eingefärbt.\n",
    "\n",
    "Bei den Autos mit einem Preis kleiner oder gleich 16376.50 EUR müssen wir nicht\n",
    "weiter sortieren bzw. weitere Fragen stellen. Da aus diesem Knoten keine Zweige\n",
    "mehr wachsen, wird dieser Knoten auch **Blatt** (Leaf) genannt. Aber in dem\n",
    "Knoten des rechten Zweiges befinden sich fünf rote (also verkaufte) Autos und\n",
    "ein blaues (also nicht verkauftes) Auto. Wir wollen diese Autos durch weitere\n",
    "Fragen sortieren. Doch obwohl nur ein Auto (nämlich Auto 3) aus dieser Gruppe\n",
    "separiert werden soll, ist dies nicht durch nur eine einzige Frage möglich.\n",
    "Lautet die Frage: »Ist der Preis kleiner oder gleich 17300 EUR?«, dann wandern\n",
    "das rote Auto 8 und das blaue Auto 3 nach links. Wählen wir die Frage: »Ist der\n",
    "Kilometerstand kleiner oder gleich 13500 km?«, dann wandern ebenfalls Auto 3 und\n",
    "Auto 8 nach links. Beide Fragen sind also gleichwertig, welches sollen wir\n",
    "nehmen? Wir gehen nach der Reihenfolge der Eigenschaften vor. Da der\n",
    "Kilometerstand in der Tabelle in der ersten Spalte steht und der Preis in der\n",
    "zweiten Spalte, entscheiden wir uns für die Frage nach dem Kilometerstand: *»Ist\n",
    "der Kilometerstand kleiner oder gleich 13500 km?«* Alternativ könnten wir auch\n",
    "den Zufall entscheiden lassen.\n",
    "\n",
    "Im Streudiagramm (Scatterplot) wird die noch nicht eingefärbte Fläche rechts der\n",
    "vertikalen Linie 13500 km rot gefärbt. Im linken Knoten (Node) sind aber nur\n",
    "noch zwei Autos, so dass diesmal eine weitere Frage ausreicht, die beiden Autos\n",
    "in zwei Klassen zu sortieren. Wir fragen: *»Ist der Kilometerstand kleiner oder\n",
    "gleich 8198 km?«*\n",
    "\n",
    "Alle Autos sind nun durch die Fragen sortiert und befinden sich in Blättern\n",
    "(Leaves). Im Streudiagramm (Scatterplot) wird dieser Zustand kenntlich gemacht,\n",
    "indem auch die letzte verbleibende Fläche (oberhalb eines Preises von 16376.50\n",
    "EUR) links von Kilometerstand 8198 km rot und rechts davon blau eingefärbt wird.\n",
    "\n",
    "> Was ist ... ein Entscheidungsbaum?\n",
    "Ein Entscheidungsbaum (Decision Tree) ist ein Modell zur Entscheidungsfindung,\n",
    "das Daten mit Hilfe einer Baumstruktur sortiert. Die Datenobjekte starten beim\n",
    "Wurzelknoten (= Ausgangssituation) und werden dann über Knoten (=\n",
    "Entscheidungsfrage) und Zweige/Kanten (= Ergebnis der Entscheidung) in Blätter\n",
    "(= Endzustand des Entscheidungsprozesses) sortiert.\n",
    "\n",
    "### Entscheidungsbäume mit Scikit-Learn trainieren\n",
    "\n",
    "In der Praxis verwenden wir die ML-Bibliothek Scikit-Learn, um einen\n",
    "Entscheidungsbaum zu trainieren. Das Modul [Scikit-Learn →\n",
    "Tree](https://scikit-learn.org/stable/modules/tree.html) stellt sowohl einen\n",
    "Entscheidungsbaum-Algorithmus für Klassifikationsprobleme als auch einen\n",
    "Algorithmus für Regressionsprobleme zur Verfügung. Für das obige Beispiel\n",
    "Autohaus importieren wir den Algorithmus für Klassifikationsprobleme namens\n",
    "`DecisionTreeClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04984255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07426e5",
   "metadata": {},
   "source": [
    "Dann erzeugen wir ein noch untrainiertes Entscheidungsbaum-Modell und weisen es\n",
    "der Variable `modell` zu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b4910",
   "metadata": {},
   "source": [
    "Bei der Erzeugung könnten wir noch verschiedene Optionen einstellen, die in der\n",
    "[Dokumentation Scikit-Learn →\n",
    "DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "nachgelesen werden können. Zunächst belassen wir es aber bei den\n",
    "Standardeinstellungen.\n",
    "\n",
    "Als nächstes adaptieren wir die Daten aus dem Pandas-DataFrame so, dass das\n",
    "Entscheidungsbaum-Modell trainiert werden kann. Der `DecisionTreeClassifier`\n",
    "erwartet für das Training zwei Argumente. Als erstes Argument müssen die\n",
    "Eingabedaten übergeben werden, also die Eigenschaften der Autos. Als zweites\n",
    "Argument erwartet der `DecisionTreeClassifier` die Zielgröße, also den Status\n",
    "»nicht verkauft« oder »verkauft«. Wir trennen daher den Pandas-DataFrame `daten`\n",
    "auf und verwenden die Bezeichnung `X` für die Eingabedaten und `y` für die\n",
    "Zielgröße."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f17094",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']]\n",
    "y = daten['verkauft']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e9a0b",
   "metadata": {},
   "source": [
    "Als nächstes wird der Entscheidungsbaum trainiert. Dazu wird die Methode\n",
    "`.fit()` mit den beiden Argumenten `X` und `y` aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde634de",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e16bcd",
   "metadata": {},
   "source": [
    "Jetzt ist zwar der Entscheidungsbaum trainiert, doch wir sehen nichts. Als\n",
    "erstes überprüfen wir mit der Methode `.score()`, wie gut die Prognose des\n",
    "Entscheidungsbaumes ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modell.score(X,y)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ec339",
   "metadata": {},
   "source": [
    "Eine 1 steht für 100 %, also alle 10 Autos werden korrekt klassifiziert. Dazu\n",
    "hat der `DecisionTreeClassifier` basierend auf den Eingabedaten `X` eine\n",
    "Prognose erstellt und diese Prognose mit den echten Daten in `y` verglichen. Für\n",
    "die Trainingsdaten funktioniert der Entscheidungsbaum also perfekt. Ob der\n",
    "Entscheidungsbaum ein neues, elftes Auto korrekt klassifizieren würde, kann so\n",
    "erst einmal nicht entschieden werden.\n",
    "\n",
    "### Prognosen mit Entscheidungsbäumen treffen\n",
    "\n",
    "Soll für neue Autos eine Prognose abgegeben werden, ob sie sich eher verkaufen\n",
    "lassen oder nicht, müssen die neuen Daten die gleiche Struktur wie die\n",
    "Eingangsdaten haben. Wir erzeugen daher einen neuen Pandas-DataFrame, bei dem\n",
    "die erste Eigenschaft der Kilometerstand der neuen Autos ist und die zweite\n",
    "Eigenschaft ihr Preis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "neue_autos = pd.DataFrame({\n",
    "    'Kilometerstand [km]': [7580, 11300, 20000],\n",
    "    'Preis [EUR]': [20999, 12000, 14999]\n",
    "    },\n",
    "    index=['Auto 11', 'Auto 12', 'Auto 13']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e60c10",
   "metadata": {},
   "source": [
    "Mit Hilfe der `predict()`-Methode kann dann der Entscheidungsbaum\n",
    "prognostizieren, ob die Autos verkauft werden oder nicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ef3f0",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "modell = DecisionTreeClassifier(random_state=0);\n",
    "modell.fit(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prognose = modell.predict(neue_autos)\n",
    "print(prognose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8347dd",
   "metadata": {},
   "source": [
    "Um für ein neues Auto eine Prognose abzugeben, werden zunächst den Blättern\n",
    "Klassen zugeordnet. Sind alle Blätter **rein**, d.h. befinden sich nur Autos\n",
    "einer einzigen Klasse in einem Blatt, dann bekommt das Blatt diese Klasse\n",
    "zugeordnet. Ist ein Blatt nicht rein, sondern enthält noch Autos mit\n",
    "unterschiedlichen Klassen »verkauft« oder »nicht verkauft« so wird diesem Blatt\n",
    "diejenige Klasse zugeordnet, die am häufigsten auftritt. Um diese Idee zu\n",
    "visualisieren, färben wir im Entscheidungsbaum die Blätter entsprechend rot und\n",
    "blau ein.\n",
    "\n",
    "Jedes neue Auto durchläuft jetzt die Entscheidungen, bis es in einem Blatt\n",
    "angekommen ist. Die Klasse des Blattes ist dann die Prognose für dieses Auto.\n",
    "\n",
    "Der Entscheidungsbaum prognostiziert, dass Auto 11 und Auto 12 nicht verkauft\n",
    "werden, aber Auto 13 könnte verkaufbar sein.\n",
    "\n",
    "## Zusammenfassung und Ausblick Kapitel 6.1\n",
    "\n",
    "In diesem Kapitel haben Sie den Entscheidungsbaum (Decision Tree) anhand einer\n",
    "Klassifikationsaufgabe kennengelernt. Mit Hilfe von Scikit-Learn wurde ein\n",
    "Entscheidungsbaum trainiert und dazu benutzt, eine Prognose für neue Daten\n",
    "abzugeben. Im nächsten Kapitel werden wir uns damit beschäftigen, weitere\n",
    "Einstellmöglichkeiten beim Training des Entscheidungsbaumes zu nutzen und\n",
    "Entscheidungsbäume durch Scikit-Learn visualisieren zu lassen.\n",
    "\n",
    "## 6.2 Entscheidungsbäume visualisieren und trainieren\n",
    "\n",
    "Im letzten Kapitel haben wir gelernt, wie mit Scikit-Learn ein Entscheidungsbaum\n",
    "für binäre Klassifikationsaufgaben trainiert wird. In diesem Kapitel werden wir\n",
    "uns damit beschäftigen, den trainierten Entscheidungsbaum von Scikit-Learn\n",
    "visualisieren zu lassen. Darüber hinaus lernen wir, was das\n",
    "Gini-Impurity-Kriterion ist und welche weiteren Einstellmöglichkeiten es für\n",
    "Entscheidungsbäume in Scikit-Learn gibt.\n",
    "\n",
    "### Lernziele Kapitel 6.2\n",
    "\n",
    "* Sie können einen Entscheidungsbaum mit `plot_tree` visualisieren.\n",
    "* Sie wissen, was die Angaben `samples` und `value` bei der Visualisierung des\n",
    "  Entscheidungsbaumes bedeuten.\n",
    "* Sie wissen, was das **Gini-Impurity-Kriterium** ist.\n",
    "* Sie kennen weitere Parameter für Entscheidungsbäume wie `random_state=` oder\n",
    "  `criterion=`.\n",
    "\n",
    "### Entscheidungsbäume visualisieren\n",
    "\n",
    "Im letzten Kapitel haben wir den Entscheidungsbaum für das Autohaus mit Hilfe\n",
    "des Moduls Scikit-Learn trainiert. Scikit-Learn bietet in dem Untermodul\n",
    "`sklearn.tree` nicht nur Algorithmen für Entscheidungsbäume an, sondern auch ein\n",
    "dazu passendes Visualisierungswerkzeug. Die Funktion `plot_tree` zeichnet den\n",
    "Entscheidungsbaum. Um diese Funktion auszuprobieren, wird zunächst der Datensatz\n",
    "mit den Autodaten erneut geladen, das Modell Entscheidungsbaum gewählt und\n",
    "anschließend trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d30e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Sammlung der Daten \n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': [32908, 20328, 13285, 17162, 27449, 13715, 32889,  3111, 15607, 18295],\n",
    "    'Preis [EUR]': [15960, 20495, 17227, 17851, 5428, 22772, 13581, 16793, 23253, 11382],\n",
    "    'verkauft': [False, True, False, True, False, True, False, True, True, False],\n",
    "    },\n",
    "    index=['Auto 1', 'Auto 2', 'Auto 3', 'Auto 4', 'Auto 5', 'Auto 6', 'Auto 7', 'Auto 8', 'Auto 9', 'Auto 10'])\n",
    "daten.head(10)\n",
    "\n",
    "# Auswahl des Modells: Entscheidungsbaum für Klassifikation\n",
    "modell = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Adaption der Daten\n",
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']]\n",
    "y = daten['verkauft']\n",
    "\n",
    "# Training des Modells\n",
    "modell.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651fb1f",
   "metadata": {},
   "source": [
    "Nun können wir die Funktion `plot_tree` importieren und das trainierte Modell\n",
    "visualisieren lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce046a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989dec5",
   "metadata": {},
   "source": [
    "`plot_tree` produziert eine Textausgabe und ein Diagramm. Die Textausgabe kann\n",
    "unterdrückt werden, indem hinter den Funktionsaufruf `plot_tree(modell)` ein\n",
    "Semikolon `;` gesetzt wird. Das Diagramm zeichnet wie erwartet die Baumstruktur\n",
    "vom Wurzelknoten über die Knoten und Zweige bis hin zu den Blättern. Die\n",
    "Entscheidungsfragen stehen in der erste Zeile der Knoten. Danach folgen weitere\n",
    "Angaben wie `gini`, `samples` und `value`. Um diese Angaben zu erklären,\n",
    "ergänzen wir zunächst weitere Angaben. Mit der Option `feature_names=` wird eine\n",
    "Liste mit den Eigenschaften ergänzt, die Option `class_names=` ergänzt die\n",
    "Klassenbezeichnugnen. So erhalten wir folgendes Diagramm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af139d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(modell, \n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419a962",
   "metadata": {},
   "source": [
    "Was `gini` bedeuten könnte, erschließt sich so immer noch nicht, aber die\n",
    "Angaben `samples` und `values` können so leichter von ihrer Bedeutung her\n",
    "eingeordnet werden. `samples` gibt die Anzahl der Datenobjekte an, die sich in\n",
    "diesem Knoten befinden. `values` listet auf, wie viele Datenobjekte die\n",
    "Zielgröße `nicht verkauft` (= False bzw. 0) haben und wie viele zu der Klasse\n",
    "`verkauft` (= True bzw. 1) gehören.\n",
    "\n",
    "Weitere Details zu den Optionen der `plot_tree`-Funktion finden Sie in der\n",
    "[Dokumentation Scikit-Learn →\n",
    "plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html).\n",
    "\n",
    "Als nächstes widmen wir uns der Bedeutung von `gini`.\n",
    "\n",
    "### Was ist das Gini-Impurity-Kriterium?\n",
    "\n",
    "Das Gini-Impurity-Kriterium ist ein Maß für die Unreinheit eines Datensatzes.\n",
    "Beim Beispiel mit dem Autohaus sind im Wurzelknoten fünf Autos, die nicht\n",
    "verkauft wurden, und fünf verkaufte Autos. Bei zwei Klassen ist das die maximale\n",
    "Unreinheit, die auftreten kann. Der Anteil der verkauften Autos ist genau 50 %.\n",
    "Diesem prozentualen Anteil wird das Gini-Impurity-Kriterium von 0.5 zugeordnet.\n",
    "Es gibt zwei weitere Extremfälle. Entweder sind nur verkaufte Autos im Datensatz\n",
    "(100 % verkaufte Autos) oder gar keine verkaufte Autos (0 % verkaufte Autos). In\n",
    "beiden Fällen ist der Datensatz rein, das Gini-Impurity-Kriterium ist 0. In\n",
    "allen anderen Fällen liegt das Gini-Impurity-Kriterium zwischen 0 und 0.5. Die\n",
    "Formel zur Berechnung des genauen Wertes des Gini-Impurity-Kriteriums lautet\n",
    "\n",
    "$$\\text{GI} = 1 - p^2 - (1-p)^2,$$\n",
    "\n",
    "wenn $p$ der prozentuale Anteil der verkauften Autos ist (das gilt natürlich\n",
    "allgemein für binäre Klassifikationsaufgaben und nicht nur das\n",
    "Autohaus-Beispiel).\n",
    "\n",
    "Die folgende Abbildung zeigt die konkreten Werte des Gini-Impurity-Kriteriums\n",
    "für den prozentualen Anteil an verkauften Autos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57616da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace\n",
    "\n",
    "p = linspace(0,1)\n",
    "gini = 1 - p**2 - (1-p)**2\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(x = p, y = gini,\n",
    "        title='Gini-Impurity-Kriterium',\n",
    "        labels={'x': 'prozentualer Anteil', 'y': 'Wert des Gini-Impurity-Kriteriums'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2f6dd",
   "metadata": {},
   "source": [
    "Im Diagramm können wir direkt ablesen, dass bei einem nicht verkauften Auto und\n",
    "fünf verkauften Autos ($p = 0.8\\bar{3}$) das Gini-Impurity-Kriterium den Wert\n",
    "$0.27\\bar{7} \\approx 0.278$ hat.\n",
    "\n",
    "Das Gini-Impurity-Kriterium ist sehr wichtig für das Training eines\n",
    "Entscheidungsbaumes. Der Algorithmus probiert im Hintergrund verschiedene\n",
    "Möglichkeiten durch, mit Hilfe der Entscheidungsfragen den Datensatz zu\n",
    "splitten. Zu jedem Split werden dann die zugehörigen Werte des\n",
    "Gini-Impurity-Kriteriums berechnet. Dann wählt der Algorithmus den Split aus,\n",
    "der die höchste Reinheit hat (also den niedrigsten Gini-Impurity-Wert). Gilt das\n",
    "für mehrere Splits, dann wird zufällig ein Split ausgewählt.  \n",
    "\n",
    "Neben dem Gini-Impurity-Kriterium gibt es noch weitere Bewertungsmaße, um einen\n",
    "Entscheidungsbaum zu trainieren. In Scikit-Learn sind die beiden Alternativen\n",
    "`log_less` und `entropy` für den **Shannonschen Informationsgewinn** verfügbar.\n",
    "Wir schauen uns im Folgenden an, wie diese ausgewählt werden können.\n",
    "\n",
    "### Entscheidungsbäume trainieren\n",
    "\n",
    "Der Entscheidungsbaum-Klassifikationsalgorithmus von Scikit-Learn bietet noch\n",
    "weitere Optionen an, wie die Hilfe verrät\n",
    "\n",
    "```python\n",
    "help(DecisionTreeClassifier())\n",
    "```\n",
    "\n",
    "oder in der [Dokumentation Scikit-Learn → DecisionTreeClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) nachgelesen werden kann.\n",
    "\n",
    "Sowohl bei der Initalisierung des Entscheidungsbaumes können Parameter gesetzt\n",
    "werden, als auch beim Verwenden der verschiedenen Methoden. Tatsächlich haben\n",
    "wir bereits weiter oben aus didaktischen Gründen den Parameter `random_state=0`\n",
    "bei der Initialisierung gesetzt, damit immer der gleiche Entscheidungsbaum\n",
    "entsteht. In einem echten Projekt würde dieser Parameter nie verwendet werden.\n",
    "\n",
    "Probieren Sie andere Werte für den Start des Zufallszahlengenerators aus und\n",
    "testen Sie, was sich verändert, wenn Sie andere Kriterien für das Splitting\n",
    "verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2922b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = DecisionTreeClassifier(criterion='entropy', random_state=3)\n",
    "modell.fit(X,y)\n",
    "\n",
    "plot_tree(modell, \n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f117be0",
   "metadata": {},
   "source": [
    "### Zusammenfassung und Ausblick Kapitel 6.2\n",
    "\n",
    "In diesem Kapitel haben wir das Training von Entscheidungsbäumen mit Hilfe der\n",
    "Bibliothek Scikit-Learn vertieft. Im nächsten Kapitel widmen wir uns den Vor-,\n",
    "aber auch den Nachteilen von Entscheidungsbäumen.\n",
    "\n",
    "## 6.3 Entscheidungsbäume in der Praxis\n",
    "\n",
    "Entscheidungsbäume bieten viele Vorteile, haben aber auch Nachteile, die wir in\n",
    "diesem Kapitel diskutieren werden. Darüber hinaus lernen wir Methoden kennen,\n",
    "bei Entscheidungsbäumen diese Nachteile zu reduzieren.\n",
    "\n",
    "### Lernziele Kapitel 6.3\n",
    "\n",
    "* Sie können in eigenen Worten erklären, was **Overfitting** (deutsch:\n",
    "  **Überanpassung**) ist.\n",
    "* Sie wissen, was **Underfitting** bedeutet.\n",
    "* Sie wissen, dass Entscheidungsbäume eine Tendenz zu Overfitting haben und\n",
    "  Maßnahmen zur Reduzierung von Overfitting ergriffen werden müssen.\n",
    "* Sie wissen, was **Hyperparameter** sind.\n",
    "* Sie kennen Hyperparameter der Entscheidungsbäume wie beispielsweise\n",
    "  * maximale Baumtiefe,\n",
    "  * minimale Anzahl an Datenpunkten in Knoten oder\n",
    "  * minimale Anzahl an Datenpunkten in Blättern.\n",
    "* Sie können die Hyperparameter zum **Prä-Pruning** (deutsch: vorab\n",
    "  Zurechtschneiden) geeignet wählen.\n",
    "\n",
    "### Die Tendenz von Entscheidungsbäumen zum Overfitting\n",
    "\n",
    "Entscheidungsbaummodelle bieten zahlreiche Vorteile. Ein wesentlicher Vorzug ist\n",
    "die Möglichkeit, den trainierten Entscheidungsbaum zu visualisieren, wodurch es\n",
    "leicht nachvollziehbar wird, welche Merkmale einen signifikanten Einfluss haben.\n",
    "Ein weiterer Vorteil ist ihre Effizienz bei heterogenen Daten; sowohl numerische\n",
    "als auch kategoriale Eigenschaften können problemlos verarbeitet werden.\n",
    "Entscheidungsbäume sind selbst bei unterschiedlichen Datenskalen robust und\n",
    "erfordern nur wenig Vorverarbeitung.\n",
    "\n",
    "Trotz dieser Stärken besitzen Entscheidungsbäume eine Neigung zum\n",
    "**Overfitting**. Overfitting, auch als Überanpassung bekannt, beschreibt ein\n",
    "Problem im maschinellen Lernen, bei dem ein Modell die Trainingsdaten zu genau\n",
    "lernt. Das klingt zunächst gut, aber das Modell kann dadurch seine Fähigkeit\n",
    "verlieren, Vorhersagen für neue, unbekannte Daten zu treffen. Im Gegensatz dazu\n",
    "steht das **Underfitting**, das eine zu geringe Anpassung an die Daten bedeutet\n",
    "und ebenfalls unerwünscht ist.\n",
    "\n",
    "Um uns das Problem des Overfittings zu veranschaulichen, betrachten wir erneut\n",
    "das Autohaus-Beispiel, aber diesmal mit mehr Autos. Wir lassen die Autos diesmal\n",
    "mit einer in Scikit-Learn eingebauten Funktion zur Generierung von künstlichen\n",
    "Daten erzeugen, der sogenannten `make_moons`-Funktion (siehe [Dokumentation\n",
    "Scikit-Learn →\n",
    "make_moons](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html))\n",
    "aus dem Module `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b65236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons \n",
    "\n",
    "X_array, y_array = make_moons(noise = 0.5, n_samples=50, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1d6ac",
   "metadata": {},
   "source": [
    "Damit die künstlichen Daten besser zu dem Autohaus-Beispiel passen,\n",
    "transformieren wir sie und nutzen die Pandas-Datenstrukturen, um sie effizient\n",
    "zu verwalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Transformation der Merkmalswerte in einen positiven Bereich und \n",
    "# Umwandlung in eine Integer-Matrix\n",
    "X_array = X_array + 1.2 * np.abs(np.min(X_array))\n",
    "X_array = X_array + 1.2 * np.abs(np.min(X_array))\n",
    "X_array[:,0] = np.ceil(X_array[:,0] * 30000)\n",
    "X_array[:,1] = np.ceil(X_array[:,1] * 10000)\n",
    "X = pd.DataFrame(X_array, columns=['Kilometerstand [km]', 'Preis [EUR]'], dtype=(int, int))\n",
    "\n",
    "# Zuweisung von True/False basierend auf den Kategorien 1 bzw. 0\n",
    "y_array = (y_array - 1.0) * (-1)\n",
    "y = pd.Series(y_array, name='verkauft', dtype='bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed935ab",
   "metadata": {},
   "source": [
    "Nach der Datenvorbereitung visualisieren wir diese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f963c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X['Kilometerstand [km]'], y = X['Preis [EUR]'], color=y,\n",
    "    title='Künstliche Daten Autohaus',\n",
    "    labels={'x': 'Kilometerstand [km]', 'y': 'Preis [EUR]', 'color': 'verkauft'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a87bb6",
   "metadata": {},
   "source": [
    "Das Training des Entscheidungsbaumes und dessen Visualisierung erledigt der\n",
    "folgende Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "modell = DecisionTreeClassifier(random_state=0)\n",
    "modell.fit(X,y)\n",
    "\n",
    "plot_tree(modell,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a02c79",
   "metadata": {},
   "source": [
    "Die Visualisierung offenbart zahlreiche Verzweigungen und eine schwer lesbare\n",
    "Beschriftung. Die Entscheidungsgrenzen, die im Folgenden mit\n",
    "`DecisionBoundaryDisplay` visualisiert werden, zeigen eine zu starke Anpassung\n",
    "an die Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a622e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig = DecisionBoundaryDisplay.from_estimator(modell, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\n",
    "fig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\n",
    "fig.ax_.set_title('Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d3a2e",
   "metadata": {},
   "source": [
    "Es ist fraglich, ob dieser Entscheidungsbaum nicht zu genau an die\n",
    "Trainingsdaten angepasst wurde. Der dünne blaue vertikale Streifen bei ungefähr\n",
    "97000 km ist wahrscheinlich keine sinnvolle Entscheidung, sondern eher einem\n",
    "Ausreißer geschuldet (dem Auto mit einem Kilometerstand von 97098 km und einem\n",
    "Preis von 28229 EUR). Der Entscheidungsbaum hat sich zu stark an die Daten\n",
    "angepasst. Es ist wahrscheinlich, dass dieser Entscheidungsbaum für Autos mit\n",
    "einem Kilometerstand von ungefähr 97000 km falsche Prognosen treffen wird. Wenn\n",
    "wir mit den gleichen Daten erneut einen Entscheidungsbaum trainieren lassen und\n",
    "den Zufallszahlengenerator mit dem Zustand `random_state=1` initialisieren,\n",
    "erhalten wir ein völlig anderes Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_alternative = DecisionTreeClassifier(random_state=1)\n",
    "modell_alternative.fit(X,y)\n",
    "\n",
    "fig = DecisionBoundaryDisplay.from_estimator(modell_alternative, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\n",
    "fig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\n",
    "fig.ax_.set_title('Entscheidungsgrenzen des alternativen Modells');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef5d71",
   "metadata": {},
   "source": [
    "Eine Möglichkeit, das Overfitting (Überanpassung) an die Daten zu bekämpfen, ist\n",
    "das Zurechtschneiden (Pruning) der Entscheidungsbäume. Eine andere ist, aus\n",
    "mehreren Entscheidungbäumen einen »durchschnittlichen« Entscheidungsbaum zu\n",
    "bilden. Dieses Verfahren heißt Zufallswald (Random Forest) und wird ausführlich\n",
    "in einem eigenen Kapitel behandelt werden. In diesem Kapitel betrachten wir nur\n",
    "das Zurechtschneiden der Entscheidungsbäume.\n",
    "\n",
    "### Zurechtschneiden von Entscheidungsbäumen\n",
    "\n",
    "Eine effektive Strategie zur Bekämpfung des Overfitting-Problems bei\n",
    "Entscheidungsbäumen ist das sogenannte **Pruning**, also das Beschneiden des\n",
    "Baumes. Pruning hilft, die Komplexität des Modells zu reduzieren, indem weniger\n",
    "relevante Entscheidungszweige nach bestimmten Kriterien entfernt werden. Im\n",
    "Kontext unseres Autohaus-Beispiels würde dies bedeuten, dass\n",
    "Entscheidungszweige, die beispielsweise aufgrund von Ausreißern entstanden sind,\n",
    "abgeschnitten werden. Dies könnte beispielsweise den zuvor erwähnten dünnen\n",
    "blauen Streifen bei einem Kilometerstand von ungefähr 97000 km betreffen, der\n",
    "wahrscheinlich durch einen Ausreißer entstanden ist. Durch das Entfernen solcher\n",
    "spezifischen Anpassungen kann der Entscheidungsbaum besser verallgemeinern und\n",
    "wird robuster gegenüber neuen, unbekannten Daten. Das Ergebnis ist ein Modell,\n",
    "das eine bessere Balance zwischen Anpassung an die Trainingsdaten und\n",
    "Generalisierungsfähigkeit aufweist.\n",
    "\n",
    "Für Entscheidungsbäume gibt es prinzipiell zwei Methoden des Prunings:\n",
    "**Prä-Pruning** und **Post-Pruning**. Das Prä-Pruning findet *vor* dem Training\n",
    "des Entscheidungsbaumes statt, das Post-Pruning *nach* dem Training. Die beiden\n",
    "wichtigsten Prä-Pruning-Maßnahmen sind\n",
    "\n",
    "* die Begrenzung der maximalen Tiefe des Baumes und\n",
    "* die Forderung nach einer Mindestanzahl von Datenpunkten (entweder pro Knoten\n",
    "  oder pro Blatt).\n",
    "\n",
    "Beim Post-Pruning werden im Nachhinein Knoten mit wenig Informationen aus dem\n",
    "Entscheidungsbaum entfernt oder es werden Knoten zusammengelegt. Scikit-Learn\n",
    "hat nur Prä-Pruning implementiert, so dass wir hier nicht weiter auf\n",
    "Post-Pruning eingehen.\n",
    "\n",
    "#### Prä-Pruning: Baumtiefe\n",
    "\n",
    "Wir schauen uns zunächst an, wie bei Scikit-Learn-Entscheidungsbäumen die\n",
    "maximale Tiefe festgelegt wird. Bisher haben wir das Modell ohne weitere\n",
    "Parameter initialisiert (einzige Ausnahme: wir haben ggf. den\n",
    "Zufallszahlengenerator aus didaktischen Gründen fixiert, damit die Ergebnisse\n",
    "vergleichbar sind). Nun verwenden wir bei der Initialisierung des\n",
    "DecisionTreeClassifiers das optionale Argument `max_depth=` und setzen es auf\n",
    "`1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a87c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_tiefe1 = DecisionTreeClassifier(random_state=0, max_depth=1)\n",
    "modell_tiefe1.fit(X,y)\n",
    "\n",
    "plot_tree(modell_tiefe1,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c8946",
   "metadata": {},
   "source": [
    "Eine Tiefe von 1 bedeutet, dass nur noch eine einzige Entscheidungsfrage\n",
    "gestellt wird. Das reicht nicht mehr, um die Autos in reine Blätter zu\n",
    "sortieren. Im linken Blatt sind 13 nicht verkaufte Autos und 24 verkaufte Autos,\n",
    "weshalb diesem Blatt die Kategorie »verkauft« zugeordnet wird. Im rechten Blatt\n",
    "sind 12 nicht verkaufte Autos und ein verkauftes Auto, so dass dieses Blatt\n",
    "insgesamt als »nicht verkauft« gilt. Die Visualisierung der Entscheidungsgrenzen\n",
    "zeigt, um welche Autos es sich handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = DecisionBoundaryDisplay.from_estimator(modell_tiefe1, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\n",
    "fig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\n",
    "fig.ax_.set_title('Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198a81e",
   "metadata": {},
   "source": [
    "Insbesondere die Visualisierung der Entscheidungsgrenzen zeigt aber auch, dass\n",
    "dieser Entscheidungsbaum nicht besonders gut die Daten erklärt. Der Score ist\n",
    "mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22562e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score des Entscheidungsbaumes mit Tiefe 1: {modell_tiefe1.score(X,y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870f407",
   "metadata": {},
   "source": [
    "auch nicht so gut. Daher verwenden wir nun als maximale Tiefe des Entscheidungsbaumes einen Wert von 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf85546",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_tiefe2 = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "modell_tiefe2.fit(X,y)\n",
    "\n",
    "plot_tree(modell_tiefe2,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);\n",
    "\n",
    "print(f'Score des Entscheidungsbaumes mit Tiefe 2: {modell_tiefe2.score(X,y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7aa3a2",
   "metadata": {},
   "source": [
    "Mit einem Score von 0.78 ist der Entscheidungsbaum mit einer maximalen Tiefe von\n",
    "2 zwar besser als der Baum mit einer maximalen Tiefe von 1, aber deutlich\n",
    "entfernt von dem Score 1.0 bei einer Baumtiefe von 7. Die Entscheidungsgrenzen\n",
    "sehen folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0633ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = DecisionBoundaryDisplay.from_estimator(modell_tiefe2, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\n",
    "fig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\n",
    "fig.ax_.set_title('Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2fae2",
   "metadata": {},
   "source": [
    "Was ist jetzt besser, eine maximale Tiefe von 1 oder 2? Oder doch 3 vielleicht?\n",
    "Die Einführung der maximalen Tiefe bietet den Vorteil, das Overfitting zu\n",
    "bekämpfen. Der Nachteil davon ist, dass wir jetzt einen neuen Parameter haben,\n",
    "der das Training und die Prognose des Modells bestimmt. Und für diesen Parameter\n",
    "muss ein passender Wert eingestellt werden. Solche Parameter nennt man\n",
    "**Hyperparameter**.\n",
    "\n",
    "> Was ist ... ein Hyperparameter?\n",
    "Ein Hyperparameter ist ein Parameter, der vor dem Training eines Modells\n",
    "festgelegt wird und nicht aus den Daten während des Trainings gelernt wird. Die\n",
    "Hyperparameter steuern den gesamten Lernprozess und haben einen wesentlichen\n",
    "Einfluss auf die Leistung des Modells.\n",
    "\n",
    "Kommen wir nun zu einem anderen Hyperparameter der Entscheidungsbäume, der\n",
    "Mindestanzahl von Datenpunkten.\n",
    "\n",
    "#### Prä-Pruning: Mindestanzahl Datenpunkte\n",
    "\n",
    "Genau wie der Hyperparameter zur Begrenzung der Baumtiefe wird die Mindestanzahl\n",
    "der Datenpunkte vorab bei der Initialisierung des Entscheidungsbaumes\n",
    "festgelegt. Scikit-Learn bietet wiederum zwei Möglichkeiten, über die minimale\n",
    "Anzahl von Datenpunkten den Entscheidungsbaum zurechtzuschneiden. Zum einen kann\n",
    "für die *Knoten* eine minimal erforderliche Anzahl von Datenpunkten festgelegt\n",
    "werden, ab der es erlaubt ist, durch Entscheidungsfragen weiter zu verzweigen.\n",
    "Zum anderen kann eine minimale Anzahl an Datenpunkten für jedes *Blatt*\n",
    "festgelegt werden, das am Ende der Verzweigungen erreicht werden muss.\n",
    "\n",
    "Wir probieren beide Möglichkeiten aus und vergleichen die Ergebnisse\n",
    "miteinander. Die Option zur Einstellung der Mindestanzahl pro Knoten heißt\n",
    "`min_samples_split` und die Option zur Einstellung des Mindestanzahl Datenpunkte\n",
    "pro Blatt heißt `min_samples_leaf`. Beiden optionalen Argumenten kann entweder\n",
    "ein Integer übergeben werden oder ein Float. Wird ein Integer übergeben, so ist\n",
    "damit die tatsächliche minimale Anzahl an Datenpunkten gemeint. Ein Float wird\n",
    "als Bruch interpretiert und meint die relative Anzahl der Datenpunkte. Der Bruch\n",
    "wird mit der Gesamtzahl der Datenpunkte multipliziert und dann wird auf die\n",
    "nächste ganze Zahl aufgerundet.\n",
    "\n",
    "Schauen wir uns beide Varianten an. Zunächst begrenzen wir die Knoten und\n",
    "fordern, dass sich in jedem Entscheidungsknoten mindestens sechs Datenpunkte\n",
    "befinden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_knotenbegrenzung = DecisionTreeClassifier(random_state=0, min_samples_split=6)\n",
    "modell_knotenbegrenzung.fit(X,y)\n",
    "\n",
    "plot_tree(modell_knotenbegrenzung,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);\n",
    "\n",
    "print(f'Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Knoten: {modell_knotenbegrenzung.score(X,y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a16647",
   "metadata": {},
   "source": [
    "Der Score ist 0.92. Nun fordern wir, dass in jedem Blatt mindestens sechs\n",
    "Datenpunkte verbleiben müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_blattbegrenzung = DecisionTreeClassifier(random_state=0, min_samples_leaf=6)\n",
    "modell_blattbegrenzung.fit(X,y)\n",
    "\n",
    "plot_tree(modell_blattbegrenzung,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);\n",
    "\n",
    "print(f'Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Knoten: {modell_blattbegrenzung.score(X,y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2984bf9",
   "metadata": {},
   "source": [
    "In diesem Fall erhalten wir einen Entscheidungsbaum mit einem Score von 0.82.\n",
    "Was jetzt die bessere Wahl ist -- Begrenzung der Baumtiefe oder Festlegung einer\n",
    "Mindestanzahl von Datenpunkten Knoten/Blatt -- und vor allem welche Wert der\n",
    "Hyperparameter haben soll, muss gesondert untersucht werden.\n",
    "\n",
    "## Zusammenfassung und Ausblick Kapitel 6.3\n",
    "\n",
    "In diesem Kapitel haben wir die Tendenz der Entscheidungsbäume zum Overfitting\n",
    "diskutiert. Um dem Problem des Overfittings zu begegnen, bietet Scikit-Learn die\n",
    "Möglichkeit des Prä-Prunings. Durch die Begrenzung der maximalen Baumtiefe oder\n",
    "die Festlegung einer Mindestanzahl von Datenpunkten in Knoten oder Blättern kann\n",
    "Overfitting reduziert werden. Diese zusätzlichen Parameter des\n",
    "Entscheidungsbaum-Modells werden Hyperparameter genannt und müssen adjustiert\n",
    "werden. Eine weitere Alternative, das Overfitting von Entscheidungsbäumen zu\n",
    "minimieren, bieten die Random Forests, die wir in einem späteren Kapitel\n",
    "kennenlernen werden."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
