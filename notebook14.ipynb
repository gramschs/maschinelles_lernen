{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c3da4f",
   "metadata": {},
   "source": [
    "# 14. Neuronale Netze\n",
    "\n",
    "Durch neuronale Netze, die tief verschachtelt sind (= tiefe neuronale Netze =\n",
    "deep neural network), gab es im Bereich des maschinellen Lernens einen\n",
    "Durchbruch. Neuronale Netze sind eine Technik aus der Statistik, die bereits in\n",
    "den 1940er Jahren entwickelt wurde. Seit ca. 10 Jahren erleben diese Techniken\n",
    "verbunden mit Fortschritten in der Computertechnologie eine Renaissance.\n",
    "\n",
    "Neuronale Netze bzw. Deep Learning kommt vor allem da zum Einsatz, wo es kaum\n",
    "systematisches Wissen gibt. Damit neuronale Netze erfolgreich trainiert werden\n",
    "können, brauchen sie sehr große Datenmengen. Nachdem in den letzten 15 Jahren\n",
    "mit dem Aufkommen von Smartphones die Daten im Bereich Videos und Fotos massiv\n",
    "zugenommen haben, lohnt sich der Einsatz der neuronalen Netze fúr\n",
    "Spracherkennung, Gesichtserkennung oder Texterkennung besonders.\n",
    "\n",
    "Beispielsweise hat ein junges deutsches Start-Up-Unternehmen 2017 aus einem\n",
    "neuronalen Netz zum Übersetzen Englisch <-> Deutsch eine Webanwendung entwickelt\n",
    "und ins Internet gestellt, die meinen Alltag massiv beeinflusst: DeepL.com,\n",
    "siehe\n",
    "\n",
    "> [https://www.deepl.com/en/blog/how-does-deepl-work](https://www.deepl.com/en/blog/how-does-deepl-work)\n",
    "\n",
    "Mittlerweile beherrscht DeepL 23 Sprachen, siehe auch den Wikipedia-Artikel zu\n",
    "DeepL:\n",
    "\n",
    "> [https://de.wikipedia.org/wiki/DeepL](https://de.wikipedia.org/wiki/DeepL)\n",
    "\n",
    "## 14.1 Mehrschichtiges Perzeptron\n",
    "\n",
    "### Lernziele\n",
    "\n",
    "* Sie wissen, was ein **Multilayer-Perzeptron** (MLP), also ein mehrschichtiges\n",
    "  Perzeptron, ist.\n",
    "* Sie können den Begriff **Deep Learning** erklären.\n",
    "* Sie können mit Scikit-Learn ein neuronales Netz trainieren.\n",
    "\n",
    "### Viele Perzeptronen sind ein neuronales Netz\n",
    "\n",
    "In einem vorhergehenden Kapitel haben wir das Perzeptron, ein künstliches Neuron\n",
    "kennengelernt. Schematisch können wir es folgendermaßen darstellen:\n",
    "\n",
    "![Schematische Darstellung eines Perzeptrons](https://gramschs.github.io/book_ml4ing/_images/perceptron.svg)\n",
    "\n",
    "Jedes Eingangssignal wird mit einem Gewicht multipliziert. Anschließend werden\n",
    "die gewichteten Eingangssignale summiert. Übersteigt die gewichtete Summe einen\n",
    "Schwellenwert, feuert sozusagen das künstliche Neuron. Das Ausgabesignal wird\n",
    "aktiviert.\n",
    "\n",
    "Mathematisch gesehen, wurde nach dem Bilden der gewichteten Summe die\n",
    "Heaviside-Funktion angewendet. Im Kapitel über die logistische Regression haben\n",
    "wir bereits gelernt, dass auch andere Funktionen zum Einsatz kommen können. Bei\n",
    "der logistischen Regression wird beispielsweise die Sigmoid-Funktion verwendet.\n",
    "Bei neuronalen Netzen sind insbesondere die\n",
    "[ReLU-Funktion](https://de.wikipedia.org/wiki/Rectifier_(neuronale_Netzwerke))\n",
    "(rectified linear unit)\n",
    "\n",
    "![ReLU-Funktion](https://gramschs.github.io/book_ml4ing/_images/plot_relu_function.svg)\n",
    "\n",
    "und der [Tangens hyperbolicus](https://de.wikipedia.org/wiki/Tangens_hyperbolicus_und_Kotangens_hyperbolicus)\n",
    "\n",
    "![Tangens hyperbolicus](https://gramschs.github.io/book_ml4ing/_images/plot_tanh_function.svg)\n",
    "\n",
    "häufig eingesetzte Aktivierungsfunktionen.\n",
    "\n",
    "Oft werden diese beiden Schritte -- Bilden der gewichteten Summe und Anwenden\n",
    "der Aktivierungsfunktion -- in einem Symbol gemeinsam dargestellt, wie in der\n",
    "folgenden Abbildung zu sehen.\n",
    "\n",
    "![Vereinfachte schematische Darstellung eines Perzeptrons](https://gramschs.github.io/book_ml4ing/_images/neuron.svg)\n",
    "\n",
    "Tatsächlich sind sogar häufig Darstellungen verbreitet, bei denen nur noch durch\n",
    "die Kreise das Perzeptron oder das künstliche Neuron symbolisiert wird.\n",
    "\n",
    "![Symbolbild eines Perzeptrons bzw. eines künstlichen Neurons](https://gramschs.github.io/book_ml4ing/_images/neuron_symbolisch.svg)\n",
    "\n",
    "Die Idee des mehrschichtigen Perzeptrons ist es, eine oder mehrere\n",
    "Zwischenschichten einzuführen. In dem folgenden Beispiel wird eine\n",
    "Zwischenschichtmit zwei Neuronen eingeführt:\n",
    "\n",
    "![Ein mehrschichtiges Perzeptron (Mulitilayer Perceptron)](https://gramschs.github.io/book_ml4ing/_images/MLP_1layer_2neurons.svg)\n",
    "\n",
    "Es können beliebig viele Zwischenschichten eingeführt werden. Jede neue\n",
    "Zwischenschicht kann dabei unterschiedliche Anzahlen von Neuronen enthalten.\n",
    "Insgesamt nennen wir die so entstehende Rechenvorschrift **mehrschichtiges\n",
    "Perzeptron** oder **Multilayer Perceptron** oder **neuronales Netz**.\n",
    "\n",
    "### Viele Schichten = Deep Learning\n",
    "\n",
    "Bei neuronalen Netzen werden viele Schichten mit vielen Neuronen in die\n",
    "Rechenvorschrift einbezogen. Das führt dazu, dass vor allem sogenannte tiefe\n",
    "neuronale Netze, also solche mit vielen Schichten, extrem leistungsfähig sind.\n",
    "Umgekehrt benötigen neuronale Netze aber auch eine große Anzahl an\n",
    "Trainingsdaten mit guter Qualität.\n",
    "\n",
    "Die Firma Linguee verfügte genau über solche Deutsch-Englisch-Übersetzungen.\n",
    "2017 trainierten Mitarbeiter dieses Unternehmens auf Basis dieser Übersetzungen\n",
    "ein neuronales Netz, das die bisher dahin existierenden Übersetzungsdienste von\n",
    "beispielsweise Google Translate bei Weitem übertraf. 2022 wurde das daraus\n",
    "gegründete Start-Up DeepL zum sogenannten Einhorn, also zu einem Start-Up, das\n",
    "mit mehr als 1 Milliarde Dollar bewertet wird (siehe\n",
    "[Artikel](https://www.faz.net/aktuell/wirtschaft/deepl-der-online-uebersetzungsdienst-wird-zum-einhorn-18467883.html)).\n",
    "\n",
    "## 14.2 Neuronale Netze mit Scikit-Learn\n",
    "\n",
    "### Lernziele Kapitel 14.2\n",
    "\n",
    "* Sie können mit Scikit-Learn ein neuronales Netz zur Klassifikation trainieren.\n",
    "\n",
    "### Neuronale Netze zur Klassifikation\n",
    "\n",
    "Schauen wir uns an, wie das Training eines tiefen neuronalen Netzes in\n",
    "Scikit-Learn funktioniert. Dazu benutzen wir aus dem Untermodul\n",
    "``sklearn.neural_network`` den ``MLPClassifier``, also ein\n",
    "Multi-Layer-Perzeptron für Klassifikationsaufgaben:\n",
    "\n",
    "> [https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\n",
    "\n",
    "Wir benutzen künstliche Daten, um die Anwendung des MLPClassifiers zu\n",
    "demonstrieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "\n",
    "# Generiere künstliche Daten\n",
    "X, y = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "# Konvertierung in ein DataFrame-Objekt für Plotly Express\n",
    "df = pd.DataFrame({\n",
    "    'Feature 1': X[:, 0],\n",
    "    'Feature 2': X[:, 1],\n",
    "    'Category': pd.Series(y, dtype='category')\n",
    "})\n",
    "\n",
    "# Visualisierung\n",
    "fig = px.scatter(df, x='Feature 1', y='Feature 2', color='Category',\n",
    "                 title='Künstliche Daten')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Auswahl des Models\n",
    "# solver = 'lbfgs' für kleine Datenmengen, solver = 'adam' für große Datenmengen, eher ab 10000\n",
    "# hidden_layer: Anzahl der Neuronen pro verdeckte Schicht und Anzahl der verdeckten Schichten\n",
    "model = MLPClassifier(solver='lbfgs', hidden_layer_sizes=[5, 5])\n",
    "\n",
    "# Split Trainings- / Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validierung \n",
    "score_train = model.score(X_train, y_train)\n",
    "score_test = model.score(X_test, y_test)\n",
    "print(f'Score für Trainingsdaten: {score_train:.2f}')\n",
    "print(f'Score für Testdaten: {score_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2826c",
   "metadata": {},
   "source": [
    "Funktioniert gar nicht mal schlecht :-) Wir zeichen die Entscheidungsgrenzen\n",
    "ein, um zu sehen, wo das neuronale Netz die Trennlinien zieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "# Create grid for contour plot\n",
    "gridX, gridY = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\n",
    "gridZ = model.predict_proba(np.column_stack([gridX.ravel(), gridY.ravel()]))[:, 1]\n",
    "Z = gridZ.reshape(gridX.shape)\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = go.Scatter(x=df['Feature 1'], y=df['Feature 2'], mode='markers',\n",
    "                     marker=dict(color=df['Category'], colorscale='BlueRed_r'))\n",
    "\n",
    "# Create contour plot\n",
    "contour = go.Contour(x=np.linspace(-1.5, 1.5, 50), y=np.linspace(-1.5, 1.5, 50), z=Z, \n",
    "                     opacity=0.2, colorscale='BlueRed_r')\n",
    "\n",
    "# Create figure and add plots\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "fig.add_trace(scatter)\n",
    "fig.update_layout(title='Künstliche Messdaten und Konturen des Modells',\n",
    "                  xaxis_title='Feature 1',\n",
    "                  yaxis_title='Feature 2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be975dd",
   "metadata": {},
   "source": [
    "Im Folgenden wollen wir uns ansehen, welche Bedeutung die optionalen Parameter\n",
    "haben. Dazu zunächst noch einmal der komplette Code, aber ohne einen Split in\n",
    "Trainings- und Testdaten. Probieren Sie nun unterschiedliche Werte für die\n",
    "Architektur der verdeckten Schicht 'hidden_layer_sizes' aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741646e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setze verschiedene Werte für die Architektur der verdeckten Schicht\n",
    "my_hidden_layers = [10,10]\n",
    "\n",
    "# erzeuge künstliche Daten\n",
    "X,y = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "# Auswahl des Model\n",
    "model = MLPClassifier(solver='lbfgs', hidden_layer_sizes=my_hidden_layers)\n",
    "\n",
    "# Training und Validierung\n",
    "model.fit(X, y)\n",
    "print('Score: {:.2f}'.format(model.score(X, y)))\n",
    "\n",
    "# Visualisierung\n",
    "# Create grid for contour plot\n",
    "gridX, gridY = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\n",
    "gridZ = model.predict_proba(np.column_stack([gridX.ravel(), gridY.ravel()]))[:, 1]\n",
    "Z = gridZ.reshape(gridX.shape)\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = go.Scatter(x=df['Feature 1'], y=df['Feature 2'], mode='markers',\n",
    "                     marker=dict(color=df['Category'], colorscale='BlueRed_r'))\n",
    "\n",
    "# Create contour plot\n",
    "contour = go.Contour(x=np.linspace(-1.5, 1.5, 50), y=np.linspace(-1.5, 1.5, 50), z=Z, \n",
    "                     opacity=0.2, colorscale='BlueRed_r')\n",
    "\n",
    "# Create figure and add plots\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "fig.add_trace(scatter)\n",
    "fig.update_layout(title='Künstliche Messdaten und Konturen des Modells',\n",
    "                  xaxis_title='Feature 1',\n",
    "                  yaxis_title='Feature 2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f05e44",
   "metadata": {},
   "source": [
    "Wie Sie sehen, ist es schwierig, eine gute Architektur des neuronalen Netzes (=\n",
    "Anzahl der Neuronen pro verdeckter Schicht und Anzahl verdeckter Schichten) zu\n",
    "finden. Auch fällt das Ergebnis jedesmal ein wenig anders aus, weil\n",
    "stochastische Verfahren im Hintergrund für das Trainieren der Gewichte benutzt\n",
    "werden. Aus diesem Grund sollten neuronale Netze nur eingesetzt werden, wenn\n",
    "sehr große Datenmengen vorliegen und dann noch ist das Finden der besten\n",
    "Architektur eine große Herausforderung.\n",
    "\n",
    "### Zusammenfassung und Ausblick Kapitel 14.2\n",
    "\n",
    "Das Training eines neuronalen Netzes erfordert sehr viele Daten und\n",
    "Hyperparameter-Tuning. In dieser Vorlesung haben wir nur das Prinzip der\n",
    "neuronalen Netze kennengelernt. Weiterführende Details gehören in eine\n",
    "eigenständige Vorlesung »Deep Learning«."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
